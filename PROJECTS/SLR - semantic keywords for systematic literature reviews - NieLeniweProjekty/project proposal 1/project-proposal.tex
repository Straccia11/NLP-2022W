%
% File main.tex
%
% Contact: car@ir.hit.edu.cn, gdzhou@suda.edu.cn
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}

%\setlength\titlebox{5cm}

% You can expand the title box if you need extra space
% to show all the authors. Please do not make the title box
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{SLR - semantic keywords for systematic literature reviews\\Project Proposal for NLP Course, Winter 2022}

\author{First Author: Michał Gozdera \\
  Warsaw University of Technology \\
  {\tt 01142172@pw.edu.pl} \\\And 
  Second Author: Małgorzata Hadasz \\
  Warsaw University of Technology \\
  {\tt 01156169@pw.edu.pl} \\\AND 
  Third Author: Krystian Kurek \\
  Warsaw University of Technology \\
  {\tt 01121582@pw.edu.pl} \\\And
  supervisor: Anna Wróblewska\\
  Warsaw University of Technology \\
  {\tt anna.wroblewska1@pw.edu.pl}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Nowadays, the
rapid increase in knowledge and the amount of performed research in numerous domains (like Computer Science and Medicine) causes the need for solutions designed to automatically segregate, organize and find created papers and publications. Key aspect of such frameworks is to detect the topic of a given article, and connect the discovered subject with domain-specific concepts. Hence, the aim of our project is to address the question of finding semantic keywords for systematic literature overview. Namely, we propose different solutions to extract keywords from medical papers abstracts, tag these keywords with ontologies concepts and choose the best tags based on disambiguation techniques.

For keywords extraction, we focus mainly on BERTopic model, but other algorithms (like LDA) can possibly be compared.

One approach to keywords tagging is going to be performed with the use of NBCO annotator (standard and simple solution) while the other will be implemented from scratch with the use of words embedding concept (\textit{word2vec}).  

Tag disambiguation techniques will rely on Closest Sense method, adjusted to our problem statement. 

To the best of our knowledge, there is currently no state-of-the-art solution combining three functionalities mentioned above, and taking advantage of the latest NLP solutions. 


\end{abstract}

\section{Introduction}




Medicine scientific area is characterized by a rapid pace of creating new literature. Every year, numerous new papers in different medical domains are produced. On the other hand, reliable research requires authors to be familiar with current achievements, which in turns causes the need for effective and exact literature searching. 

The scientific goal of our research is to create a solution that would help researchers and people interested in medical papers to find the most suitable scientific publications according to their needs. 

The research question we want to address is whether the solution described above can be prepared with the use of the latest achievements in NLP domain. Our proposition is based on several techniques, including keywords extractors, ontology-based taggers and disambiguation algorithms. 

\section{Current solutions and state-of-the-art}

Currently available solutions are not specifically directed into the aim we presented. There exist state-of-the-art solutions performing specific parts of what we are going to implement, but the entire process itself is not well investigated in our opinion. 

Regarding keywords extraction, Latent Dirichlet Allocation - LDA  \cite{LDA} is one of the state-of-the-art algorithms, however currently it is usually replaced by models utilizing modern words embedding techniques, like BERTopic \cite{BERTopic}. 

For a long time, for ontology based tagging in medical data, simple solutions, like NCBO  annotator \cite{NCBO} were used. Recently, more sophisticated approaches (like ScispaCy \cite{scispacy}) appeared. What is more, words embedding idea is getting more and more interest in the NLP filed, actually being the current state-of-the-art word representation (mainly because of its high performance and important properties, like preserving semantic meaning). However, to the best of our knowledge, there is no any state-of-the-art embedding based annotator provided for the use with specific medical ontologies. 

Ontology tag disambiguation is according to our research the least explored part of the solution. There are not many papers approaching this topic, most of them treating disambiguation as a side part of other solutions (\cite{taggerOne}, \cite{dis_side}). An algorithm that seems to fit the needs of our solution best is Closest Sense method
\cite{ClosestSense}.


\section{Significance of the project}

According to current state-of-the-art research it is clear, that there is no obvious solution for semantic keywords for systematic literature reviews problem available. Based on our research, there exist models and algorithms than can be successfully modified and then combined together to create a well-performing method tackling the scientific problem we describe. 

The pioneering nature of our project include not only combing existing solutions into one, designed for specific problem, but also innovative modifications of existing methods and implementing our own ideas from scratch.

We aim to contribute to the research filed in two areas:
\begin{itemize}
    \item research resulting in creating a method of extracting semantic keywords for systematic literature reviews, based on recent NLP achievements,
    \item developing an actual solution and implementation that can be successfully incorporated into research community and improve the quality and speed of research work.
\end{itemize}

\section{Concept and work plan}
In this section we describe the project analysis and time scheduling. The main milestones and goals are shown. Moreover, we present the results of the preliminary research and  risk analysis.  
\subsection{Project activities and timeline}
We divided our project into 3 main parts, that are presented in the table \ref{project-act}.
\begin{table}[h]
\begin{center}
\begin{tabular}{|l|rl|}
\hline \bf Date & \bf Stage name & \bf Description\\ \hline
4.11.2022 & Project proposal & \begin{tabular}{@{}l@{}l@{}}literature review,\\ solution concept \\ and proposal\end{tabular}\\ \hline
18.11.2022 & Proof of concept & \begin{tabular}{@{}l@{}@{}}exploratory data analysis\\ and preliminary \\ machine learning models \end{tabular}\\ \hline
9.12.2022 &  Final project & \begin{tabular}{@{}l@{}}full solution\\ and prepared product\end{tabular}\\
\hline
\end{tabular}
\end{center}
\caption{\label{project-act}Project activity and timeline }
\end{table}

\subsection{Specific research goals}

We establish the following research goals for the project:
\begin{itemize}
    \item acquiring wide knowledge about current state-of-the art methods in NLP domain, especially in semantic keywords for systemic literature reviews field,
    \item testing different solutions currently available and adapting them to the above need,
    \item combining existing methods for keywords retrieval, keywords ontology annotating and tags disambiguation into one, working solution,
    \item creating new algorithms for keywords ontology annotating and tags disambiguation.
\end{itemize}

\subsection{Results of preliminary research}

Introductory research resulted in gathering knowledge about state-of-the-art methods than can be incorporated into our solution. They are described in section 2.
Apart from reading about particular solutions, we also tested and verified existing implementations:
\begin{itemize}
	\item Keywords extraction - the LDA algorithm is available in \textit{sklearn} library \cite{sklearn_lda}; BERTopic can be found in \textit{bertopic} package \cite{pypi_bertopic},

	\item Keywords tagging – for NCBO annotator, the REST API is available \cite{ncbo_rest}, so our solution is going to use it via HTTP connection; there is no concrete implementation of embedding-based annotations with medical ontologies that would satisfy our need, so this part will be implemented from scratch based on \textit{word2vec} implementation (\textit{gensim} package \cite{pypi_genism}),

	\item Tags disambiguation – since we are going to use a modified version of Closest Sense method, we are going to implement it from scratch.

\end{itemize}

The main result of the preliminary research is that currently available solutions (with quite a few modifications) should allow us to develop a fully usable method for extracting semantic keywords. However, there is not any specific algorithms pipeline (combining all above methods) available now. Creating one will be the goal of our project.  

To develop and test our solution, we planned to use the MedMentions data set \cite{MedMentions}. This resource provides access to over 4 000 articles (titles and abstracts) published in PubMed. Each article is annotated with UMLS \cite{UMLS} concepts by professional annotators with rich experience in biomedical content. We will treat those articles and annotations as the Gold Standard. In the MedMentions' release article, they have mentioned other relevant corpora that we can use in case of any problems with MedMentions. 


In part of our solution we need to provide the ontology. We plan to test a few of them. The main is the one, that is used in the dataset, but we also decide to try different ones:
\begin{itemize}
    \item \textit{UMLS} \cite{UMLS}
    \item \textit{The Human Disease Ontology} \cite{10.1093/bioinformatics/btaa1057}
    \item \textit{International Classification of Diseases, Version 10} \cite{conf/ic3k/MollerSBEDS10}.
\end{itemize}
Although, we might change them, if we find more suitable ones, or the aforementioned ontologies won't be satisfying. 
\subsection{Risk analysis}
The risk analysis can be divided into 4 parts. The risk of the data, the risk of the algorithms, the time shortage, and the risk of the team.

The first one may include data leakage, data removal, or the change of data privacy policy. We don’t use any vulnerable information (etc. personal numbers), therefore the risk of leakage is not high and if it occurs it wouldn’t affect anyone. We make use of open-source data sets and ontologies. Consequently, we would be affected by their removal or the change of privacy policy. To prevent it, where possible, we use multiple ontologies and datasets. As a result of that, our algorithm can work with only part of those data. 

The second hazard is the risk of the algorithms. As in the previous point, the framework owners might change the rights to the algorithm (make it private), or the tested approach might not bring the expected results. The former might be solved by using other similar frameworks, or by developing only the part of the project. Another approach might be to develop a similar algorithm from scratch. However, it would significantly enlarge our project and might lead to a lack of time for the rest of the development. The latter might be caused by various factors. The computational power, that we have, might be too low and would not allow us to train the algorithm for the desired amount of time. The pre-trained models might not be suited for the kind of data we are using, the frameworks might not work with themselves properly or the results might not be satisfactory due to other, unrecognized causes. 

The next risk is time shortage. Due to various factors, we might not have enough time to finish the project. The factors might be those mentioned in this analysis, underestimating the scope of the project, or other previously unrecognized ones. In case of a time shortage, we might develop only part of the solution and finish the project in the next assignment. 

The last risk is linked to the team. We might be affected by a mistake made by a team member, experience communication issues, or a part of the team might want to leave the project. The first one is for instance deletion of an important part of the project, to prevent it we use a shared repository and commit changes after every important change. Moreover, we control the work of our coworkers to detect possible mistakes. 

The aforementioned issues are the main ones, that we might experience. Other, unrecognized issues might occur. We would undertake all possible actions to prevent them.

\section{Approach \& research methodology}

Our approach and research methodology consist of several steps. 

Firstly, we aim to perform a thorough research in both general NLP and methods specific to our problem. This part of the project is almost finished and this report states its results. Of course, during the next phases of the project, subsequent research activities probably occur, since the development process is an iterative task. 

The second part of the problem investigation is to test various implementations of currently available methods. As described above, this part is also done. 

Next, we aim to prepare a Proof of Concept (PoC) solution that will utilize some of concepts included in previous sections. It will be probably composed of ready-to-use solutions, like BERTopic + NCBO annotator. Its aim is to illustrate the way system will work.

Then, we intend to prepare the final solution, including all methods described in previous sections. Since the project incorporates a research rather than development approach, we reserve that some of the planned methods may change.
Each stage of the project is going to be presented in front of other researchers working on similar projects as well as the project supervisor. Consultations with the supervisor are planned through all stages.    



\section{Methods, techniques, devices to be used in research}
In this section we describe methods and techniques used in developing the project solution.
\subsection{Keyword extraction}
To detect the main concepts of the given documents, we decided to use Topic Modeling. It is an unsupervised machine learning method, that scans a set of documents and clusters them into groups represented by similar abstract topics. The conventional technique LDA \cite{LDA} treats a document as a bag-of-words. Consequently, it loses the context and the order of the words. To prevent order loss and profit from the context of the given word, text embedding techniques have been used in various tasks. In recent years they became popular in the topic modeling field. Therefore, we decided to use BERTopic \cite{BERTopic}.

\subsubsection{LDA}
The Latent Dirichlet Allocation is a generative probabilistic model for finding hidden topics in the given corpora, proposed in \cite{LDA}. It makes a few assumptions:
\begin{enumerate}
    \item topics are the statistically significant words in given corpora,
    \item documents are a mixture of topics,
    \item topics are a mixture of words.
\end{enumerate}
Based on them, LDA calculates the probability density of topics in the document. 

Before performing the algorithm pre-processing is needed, words need to be tokenized and a number of expected topics need to be given (in this work it is going to be denoted as Q). After that, the word-document matrix is created. This matrix is then divided into two matrices: document-topic and topic-word one. 

LDA is an iterative process. In the first iteration, the randomly selected topics are assigned to each word. After that, LDA tries to optimize the results. In order to do this, it examines each word separately. Assuming that all assigned topics, apart from the current one, are correct. LDA tries to find the best topic for a given word. To do this it calculates 2 probabilities;
\begin{enumerate}
\item p1: proportion of words in a given document with a given topic (q),
\item p2: proportion of the documents in which the word (w) has the topic q assigned.
\end{enumerate}
Using those probabilities, it detects the most relevant topic for a given word and reassigns it.

For each word in each document, the procedure is repeated, until a steady solution is found. At the  end, the list of Q tuples containing the topic number and the list of most informative terms with their probability is given. LDA doesn't interpret topics, this step needs to be performed manually ( it only provides the topic number and the informative words, user needs to add the topic description/ name if needed). 

\subsubsection{BERTopic}
In this project we use the \cite{pypi_bertopic} BERTopic framework implementation.

To generate topic representation, BERTopic goes through 3 main steps. 

First, it embeds documents in order to create their representation in vector space and compare their semantic meaning. As a default, it uses Sentence-BERT (SBERT) framework \cite{reimers-2019-sentence-bert}, which enables converting sentences into vector representation using a pre-trained language model.
SBERT is an extension to the traditional BERT model, for which calculating the sentence probability is a very time-consuming task. As authors of the \cite{DBLP:journals/corr/abs-1908-10084} claim, by adding the pooling operation at the output of the BERT, the time of finding the most similar sentence pair in a collection of 10000 sentences was reduced from 65 hours to 5 seconds. Therefore, the BERTopic framework, by default, makes use of the SBERT model. It also allows using other pre-trained sentence embedding or custom models. 

Subsequently, it performs clustering. Due to high space dimensionality, calculating the distance might become ill-defined. Therefore, to reduce dimensionality UMAP \cite{https://doi.org/10.48550/arxiv.1802.03426} algorithm is used. The reduced embeddings are clustered using HDBSCAN \cite{McInnes2017}. The BERTopic framework allows changing both dimensionality reduction and clustering algorithms. The aforementioned techniques are used as default methods.

The last step is finding the topic representation. As a default, the modified TF-IDF procedure is used. The original procedure combines term and inverse document frequency: \begin{equation}
  W_{t,d} = tf_{f,d}\dot log(\frac{N}{df_t}) ,
\end{equation}
where $tf_{t,d}$ is the frequency of the term $t$ in document $d$, N is the number of documents and $df_t$ is a document frequency that shows how much information the term provided in the document. In BERTopic this procedure is generalized to clusters of documents. Firstly, all documents in the cluster are concatenated, then TF-IDF is modified and obtained by the formula:
\begin{equation}
W_{t,c} = tf_{f,c}\dot log(1+\frac{A}{tf_t}),
\end{equation}
where $tf_t,c$ is a frequency of the term $t$ in the class $c$. $C$ is concatenated into one document collection of the documents from the same cluster. $Tf_t$ is a class frequency, measuring how much information the term provides to a class. By using the modified TF-IDF formula the importance of the words in a cluster, rather then in the document, is modeled. 

\subsection{Tagging tools}
Keywords, extracted in the previous step, might incorporate different names to describe the same concepts. Therefore, to make use of them it is essential to perform mapping into existing ontologies. The ontology provides the standardized, homogenous, and informative concept, that describes the given keyword. The task of tagging the word with an entity existing in the ontology is called entity normalization, entity grounding, or entity categorization. As an example of the biomedical entity grounding, we will use the Onto-Biotope Ontology \cite{Nédellec2018}. Given the words "pediatric", "respiratory" and "children less than 2 years", we aim to find the appropriate tags in the ontology. For the first two words, the task is relatively simple. "Pediatric" ought to be linked to "pediatric patient" and "respiratory" to the "respiratory tract part". Both of those examples are lexically similar. In the last case, the linking is not that trivial. "Children less than 2 years" should be tagged as a "pediatric patient", even though the lexical similarity doesn't exist. The given example was derived from \cite{Karadeniz2019}.  Moreover, in the biomedical domain, the number of semantic categories is greater than the entities mentioned in available training data sets. For example, Onto-Biotope ontology consists of 2221 categories, while only 747 of them were mentioned in the training data set. Therefore, we decided to use unsupervised annotation techniques. In this section, we described two approaches tested in our solution. The first is NCBO tagger, which annotates data based mostly on direct string matching. The second, which uses word embeddings to link entities using word. 

\subsubsection{NCBO tagger}
NCBO tagger \cite{NCBO} is a result of an initiative to construct a solution for annotating biomedical data with the use of a great number of ontologies. At the time of releasing the solution it used over 200 ontologies and this number is constantly increasing. 

The way NCBO tagger works is simple, yet in many cases powerful enough. It uses a few steps to tag each token of an input free-text. 

First of all, a direct string matching is performed. The dictionary of ontologies concepts is used for this purpose. It is constructed by pooling all concept
names or other string forms (synonyms, labels) that syntactically
identify concepts. Then, tokens from the input string are matched to this dictionary entries.

Second step is performed by \textit{is\_a transitive closure}, which aims to explore the relations in ontologies, namely for a given matched concept it searches its subsequent ancestors in the parent-child hierarchy and can match them as tags for a given token as well. The number of ancestors to look through is paramterizable. 

Next, an \textit{ontology-mapping component} tries to find relations between different ontologies, e.g., when a given concept is matched for a token, it can be linked to a respective concept in another ontology, and the ontology can also be traversed. 

As a result, NCBO produces quite a lot of tags for each input text token. Our first trials showed that they are usually relevant, however often to many of them is generated. Hence the need to select the most suitable tag and possibly perform disambiguation.
\subsubsection{Word2Vec}

In order to work with word data, we have to represent it as numbers, preferably vectors. One of the encoding methods is one-hot encoding. This method allows us to change words to sparse vectors. There are many issues with this approach, for example: 
\begin{itemize}
    \item the distance between vectors is always the same,
    \item cosine similarity between vectors is always zero.
\end{itemize}


It is common to use Word2vec to overcome those issues (presented in \cite{Word2vec}). This mechanism allows us to link word literals to dense vectors. Those vectors have valuable properties, such as meaningful usage of cosine metric. It is worth mentioning that if we take the embedding of the word "king" and subtract from that embedding of the word "man", and then we add the embedding of the word "woman" - we shall get the embedding of the word "queen". 
The concept is based on a simple neural network with the following architecture: 
\begin{enumerate}
    \item First layer with weights' matrix of size  VxN with linear activation (projection layer).
    \item Output layer with weights' matrix of size NxV with softmax activation. 
\end{enumerate}


Where: N is a parameter - the size of the hidden layer (also the size of embedding), and V is the size of the vocabulary. 
We can train this architecture with two approaches: 
\begin{itemize}
    \item  Continuous Bag of Words (CBOW) - we want to train the neural network to predict the target word given its neighboring words. Projections from those words are averaged. 
    \item  Continuous Skip-gram - we want to train the neural network to predict neighboring words. The output of the second layer and the corresponding error are calculated separately for every neighboring word.
\end{itemize}

Those two approaches are illustrated on Figure \ref{fig:cbow_skip_gram}.

The number of neighboring words is a parameter. When we choose this number to be equal to 5, we will choose five words from the future and the past i. e. ten neighboring words. 

\textbf{Note}: In this description, we focused on the high-level idea behind Word2Vec, not on specific implementation issues or optimization techniques.

\begin{figure*}[!h]
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/cbow_skip_gram.png}
 \caption{Illustration of CBOW and Skip-gram models \cite{Word2vec}.}
 \label{fig:cbow_skip_gram}
 \centering
\end{figure*}


\subsubsection{Words embedding tagger}
The approach mentioned in \cite{Karadeniz2019}  is based on the assumption that semantically similar words have similar vectors in the embedded space. 

Before computing the word embedding vectors, the preprocessing is performed. The words need to be free from stop words, and non-ASCII characters. 

Next, the word vectors are calculated, using the pre-trained model. For multi-word entities, each word is transformed separately and the average vector is calculated. After the conversion of both tagged data and ontology concepts, their similarity is measured. The authors proposed 2 similarity measurements. The cosine similarity is calculated by the given equation:
\begin{equation}
    cosine\_similarity = \frac{A B}{\parallel A \parallel \parallel B \parallel},
\end{equation}
where A and B are the vectors. 
The second metric is the word mover's distance (WMD). WMD treats tags as a weighted point in the cloud of the embedded words and calculates the minimum distance it needs to travel to a given concept.
 After performing those steps, the list of closest ontologies concepts is given. 
\subsection{Words tags disambiguation}

In free-text data, the same word can occur in different contexts and with different meanings. For example, if working with data containing information about wines, \textit{Burgundy} can refer to the name of the wine or the region in France \cite{burg_example}. Hence, it should be decided whether to tag \textit{Burgundy} with \textit{Wine name} or \textit{Country Region}. This information should be based on the context of a tagging word in an input text. If it comes to medical data, the term \textit{blood pressure} can have three senses, namely \textit{organism function}, \textit{diagnostic procedure} and \textit{laboratory or test result} \cite{ClosestSense}.

We propose a method inspired by Alexopoulou
et al.~\shortcite{ClosestSense}. It is based on selecting the sense of a given word (or in general token) that is the closest to senses of other words appearing in the context. In the following subsections we describe the original Closest Sense method (sentence-based) and the modification than can be incorporated in our solution. 

\subsubsection{Sentence-based Closest Sense method}
Let us suppose that we want to tag tokens in the sentence: \textit{I also tracked lipid profiles, HBA1C, blood pressure, body mass index, hostility and nicotine use}. As mentioned above, \textit{blood pressure} can have multiple senses since it is ambiguous - three tags are possible (assuming they are concepts of some ontology): \textit{organism function}, \textit{diagnostic procedure} and \textit{laboratory or test result}.

To decide which tag should be assigned to \textit{blood pressure}, we explore tags of other words appearing in the sentence. Let us assume that the senses of the occurring terms are \textit{laboratory procedure} (lipid profile),
\textit{gene or genome} (HBA1C), \textit{diagnostic procedure} (body mass index), \textit{mental process} (hostility) and \textit{organic chemical} (nicotine). Then for blood pressure we choose the sense that is on average closer to the senses of the co-occurring terms than the other candidate senses.

What the \textit{closeness} means can be treated in various ways. For example, semantic distances utilizing the ontologies (like subsumption distance or subtype-aware signature distance) can be used \cite{ClosestSense}. The other way could be to incorporate words embedding and investigate cosine similarity. 

\subsubsection{Keywords-based Closest Sense method}

Since our task aims to tag keywords instead of particular words in a free-text, we plan to modify the Closest Sense algorithm. 

First of all, for a given ambiguous keyword, we are going to treat other keywords extracted for a given text as the context, instead of words that occur in the same sentence. 

Secondly, in the case of our problem, each keyword is ambiguous on a similar level (all keywords will have numerous candidate tags assigned). This is a difference in regards to what Alexopoulou et al. \shortcite{ClosestSense} explored: they assumed only a given word in a sentence is ambiguous while other words have correctly assigned tags. That is why we propose the following iterative procedure: given a set $K$ of keywords $k_j$, $j=1, \ldots, |K|$ for a given document and sets $|T_j|$ of candidate tags: $t_{j,i_j}$  for $j$-th kyeword, $i_j=1, \ldots, |T_j|$ perform \textit{max\_iter} times:
\begin{enumerate}
    \item Take subsequent keyword $k_j$ and assume this keyword is ambiguous, while all other keywords have correct tags assigned (take first tag in the candidate list for these keywords). 
    \item For each candidate tag $t_{j,i_j}$ calculate the similarity distances to other keywords tags and sort the list of tags from $|T_j|$ by decreasing similarity distance. As a result, at the top of the list we have the best tag for $k_j$ according to the current state.
    \item Go to point 1. taking next keyword. 
\end{enumerate}

After $max\_iter$ iterations of above points, each $k_j$ will be considered $max\_iter$ times. This heuristic can help to choose keywords tags according to the context of the entire document. 

We plan to use similarity metric based on words embedding, but other choices mentioned earlier are possible.

\section{Methods of results analysis}
To compare tagging methods, we will use two metrics: precision, recall, and F1 score, which are defined as follows:    
\begin{equation}
    precision = \frac{TP}{TP + FP} 
\end{equation}
\begin{equation}
    recall = \frac{TP}{TP + FN} 
\end{equation}
\begin{equation}
    F1 = \frac{2}{\frac{1}{precision} + \frac{1}{recall}} 
\end{equation}

Where:
\begin{itemize}
    \item True positives (TP) - number of cases when our annotation matches annotation from golden standard dataset
    \item False postives (FP) - number of cases when our annotation doesn't match annotation from golden standard dataset
    \item False negatives (FN) - number of cases when annotation from golden standard dataset is not present in our annotations
\end{itemize}
It is hard to come up with a way to calculate the number of true negatives. That is why we choose metrics that do not rely on this particular number. 


% include your bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2015}
\bibliographystyle{acl}
\bibliography{refs}

\end{document}