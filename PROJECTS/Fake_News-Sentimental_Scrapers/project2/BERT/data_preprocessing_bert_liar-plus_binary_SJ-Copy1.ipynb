{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e31007",
   "metadata": {},
   "source": [
    "# Important Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ac1e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340b59f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a41d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")  # Punkt Sentence Tokenizer\n",
    "nltk.download(\"averaged_perceptron_tagger\")  # Part of Speech Tagger\n",
    "nltk.download(\"wordnet\")  # a lexical database of English; useful for synonyms, hyponyms, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f69fe551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1cbe4",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5205880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUR_DATASET = \"LIAR-PLUS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8dd79374",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(f'../data/{CUR_DATASET}/train2.tsv', sep='\\t', header = None)\n",
    "valid_dataset = pd.read_csv(f'../data/{CUR_DATASET}/val2.tsv', sep='\\t', header = None)\n",
    "test_dataset = pd.read_csv(f'../data/{CUR_DATASET}/test2.tsv', sep='\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0fec590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train_dataset.iloc[:, [2, 3, 15]]\n",
    "train = train.rename(columns = {2: 'label', 3: 'statements', 15: 'justification'})\n",
    "\n",
    "val = valid_dataset.iloc[:, [2, 3, 15]]\n",
    "val = val.rename(columns = {2: 'label', 3: 'statements', 15: 'justification'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9953dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [train, val]:\n",
    "    dataset['label'] = dataset['label'].replace({\n",
    "        'false' : 0,\n",
    "        'barely-true' : 0,\n",
    "        'pants-fire' : 0,\n",
    "        'half-true' : 1,\n",
    "        'mostly-true' : 1,\n",
    "        'true' : 1\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f41a9f3",
   "metadata": {},
   "source": [
    "# Some More EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "44d1480d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label            0\n",
       "statements       0\n",
       "justification    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20c24341",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [train, val]:\n",
    "    dataset = dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9302f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train[\"label\"].values.astype(int)\n",
    "val_labels = val[\"label\"].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cf746345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011876e",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ee421",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fe069c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4346120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_stopwords = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ddeb4e",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d9bea26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from num2words import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f31cfa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_text_fn = {\n",
    "    \"no_punctuation\": lambda txt: re.sub(r'[^\\w\\s]','', txt),\n",
    "    \"no_special_symbols\": lambda txt: re.sub('[$,#,&]', '', txt),\n",
    "    # \"no_digits\": lambda txt: re.sub('\\d*', '', txt),\n",
    "    \"no_www\": lambda txt: re.sub('w{3}', '', txt),\n",
    "    \"no_urls\": lambda txt: re.sub('http\\S+', '', txt),\n",
    "    \"no_spaces\": lambda txt: re.sub('\\s+', ' ', txt),\n",
    "    \"no_single_chars\": lambda txt: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', txt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5cd2a305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, pipeline = preprocessing_text_fn):\n",
    "    text = str(text)\n",
    "    for fn in pipeline.keys():\n",
    "        text = pipeline[fn](text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7276370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset in [train, val]:\n",
    "    dataset[\"statements\"] = dataset[\"statements\"].apply(preprocess_text)\n",
    "    dataset[\"justification\"] = dataset[\"justification\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "25112b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if if_stopwords:\n",
    "    for dataset in [train, val]:\n",
    "        for col in [\"statements\", \"justification\"]:\n",
    "            dataset[col] = dataset[col].str.lower().str.replace(\"’\", \"'\")\n",
    "            dataset[col] = dataset[col].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6292eec",
   "metadata": {},
   "source": [
    "### Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b4b4247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_lemmatize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e159b0b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "if if_lemmatize:\n",
    "    \n",
    "    import nltk\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    for dataset in [train, val]:\n",
    "        for col in [\"statements\", \"justification\"]:\n",
    "            dataset[col] = dataset[col].str.lower().str.replace(\"’\", \"'\")\n",
    "            dataset[col] = dataset[col].apply(lambda x: ' '.join([wnl.lemmatize(word) for word in word_tokenize(x)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "849ade27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = (train[\"statements\"] + \" \" + train[\"justification\"]).values\n",
    "val_text = (val[\"statements\"] + \" \" + val[\"justification\"]).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e8bb8",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3eebdcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e2a2ae17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device('cuda')    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5d41e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5fada532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "17dcf749",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "529662e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e6a38f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', # Use the 124-layer, 1024-hidden, 16-heads, 340M parameters BERT model with an uncased vocab.\n",
    "    num_labels = len(np.unique(train_labels)), \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False, \n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9d8b1ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                              | 0/10154 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2089 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10154/10154 [00:04<00:00, 2064.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  2089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "len_limit = 512\n",
    "LIMIT = 100_000\n",
    "\n",
    "indices = []\n",
    "train_text_filtered = []\n",
    "\n",
    "for i, text in enumerate(tqdm(train_text)):\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "    if len(input_ids) <= LIMIT:\n",
    "        train_text_filtered.append(text)\n",
    "        indices.append(i)\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "94276bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10154])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels_filtered = train_labels[indices]\n",
    "# labels_filtered.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "46221860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/jeongwonkim10516/nlp-fake-news-with-bert-99-55-top1/notebook\n",
    "\n",
    "def tokenize_map(sentence, labs='None'):\n",
    "    \n",
    "    \"\"\"A function for tokenize all of the sentences and map the tokens to their word IDs.\"\"\"\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    \n",
    "    for text in tqdm(sentence):\n",
    "        #   \"encode_plus\" will:\n",
    "        \n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        \n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            text,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            truncation='longest_first', # Activate and control truncation\n",
    "                            max_length = len_limit,           # Max length according to our text data.\n",
    "                            padding = 'max_length', # Pad & truncate all sentences.\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the id list. \n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        \n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        \n",
    "    # Convert the lists into tensors.\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    if labs != 'None': # Setting this for using this definition for both train and test data so labels won't be a problem in our outputs.\n",
    "        labels = torch.tensor(labels)\n",
    "        return input_ids, attention_masks\n",
    "    \n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ae808334",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_filtered = np.array(train_text)\n",
    "val_text_filtered = np.array(val_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "deaf7201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10154,), (10154,), (1280,), (1280,))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.shape, train_text_filtered.shape, val_text.shape, val_text_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bd4cf520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10154/10154 [00:05<00:00, 1702.62it/s]\n",
      "/tmp/ipykernel_15775/29249304.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_labels = torch.tensor(train_labels)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:00<00:00, 1709.85it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_masks = tokenize_map(train_text_filtered)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "val_input_ids, val_attention_masks = tokenize_map(val_text_filtered)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# test_input_ids, test_attention_masks= tokenize_map(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249a58e",
   "metadata": {},
   "source": [
    "## Train and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "745e5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "34763ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4fc412b810>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 10\n",
    "transformers.set_seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d21142e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ca257209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10154]), torch.Size([10154, 512]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_filtered.shape, input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c48fb067",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(input_ids, attention_masks, labels_filtered)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a427a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DataLoader\n",
    "batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  \n",
    "            sampler = RandomSampler(train_dataset), \n",
    "            batch_size = batch_size \n",
    "        )\n",
    "\n",
    "# Validation DataLoader\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            sampler = SequentialSampler(val_dataset), \n",
    "            batch_size = batch_size \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f3b8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test DataLoader\n",
    "\n",
    "# test_data = TensorDataset(test_input_ids, test_attention_masks)\n",
    "# test_sampler = SequentialSampler(test_data)\n",
    "# test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6152d",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "23503732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "edf00b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5, # args.learning_rate\n",
    "                  # eps = 1e-8 # args.adam_epsilon\n",
    "            )\n",
    "\n",
    "lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6ee0f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "total_num_steps = len(train_dataloader) * num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae540af7",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8c29ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    return accuracy_score(labels_flat, pred_flat)\n",
    "\n",
    "def flat_f1_score(predictions, labels):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "\n",
    "    return f1_score(labels_flat, pred_flat, zero_division=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf381a05",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d0f67393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4e9f7c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "----- Epoch 1 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 173.979585647583.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 346.74035024642944.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 519.6959676742554.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 692.2638189792633.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 864.7831635475159.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoc h took: 878.1250457763672\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52031\n",
      "  F1: 0.64560\n",
      "  Validation Loss: 0.69988\n",
      "  Validation took: 47.069339990615845\n",
      "\n",
      "Training...\n",
      "----- Epoch 2 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.3912980556488.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.81175684928894.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 517.6039168834686.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 690.4960925579071.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 862.9910435676575.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoc h took: 876.290992975235\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52031\n",
      "  F1: 0.64560\n",
      "  Validation Loss: 0.69988\n",
      "  Validation took: 47.076966762542725\n",
      "\n",
      "Training...\n",
      "----- Epoch 3 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.62060952186584.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.36091351509094.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 517.9514827728271.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 690.5702209472656.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 863.3893358707428.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoc h took: 876.6689507961273\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52031\n",
      "  F1: 0.64560\n",
      "  Validation Loss: 0.69988\n",
      "  Validation took: 47.0645694732666\n",
      "\n",
      "Training...\n",
      "----- Epoch 4 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.3511061668396.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.86352944374084.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 517.9004890918732.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 690.6832678318024.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 863.5662534236908.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoc h took: 876.9388036727905\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52031\n",
      "  F1: 0.64560\n",
      "  Validation Loss: 0.69988\n",
      "  Validation took: 47.063896894454956\n",
      "\n",
      "Training...\n",
      "----- Epoch 5 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.7341823577881.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.2680039405823.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 518.0673162937164.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 691.0157899856567.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 863.8188631534576.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoc h took: 877.1626839637756\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52031\n",
      "  F1: 0.64560\n",
      "  Validation Loss: 0.69988\n",
      "  Validation took: 47.070027112960815\n",
      "\n",
      "Training...\n",
      "----- Epoch 6 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.84391450881958.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.4965331554413.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 518.2050242424011.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 691.2183749675751.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 863.8666849136353.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoc h took: 877.2124202251434\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52031\n",
      "  F1: 0.64560\n",
      "  Validation Loss: 0.69988\n",
      "  Validation took: 47.06076788902283\n",
      "\n",
      "Training...\n",
      "----- Epoch 7 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.5657513141632.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.5197355747223.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 518.0293107032776.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 690.8091764450073.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 863.4715049266815.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoc h took: 876.8131885528564\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52031\n",
      "  F1: 0.64560\n",
      "  Validation Loss: 0.69988\n",
      "  Validation took: 47.06469225883484\n",
      "\n",
      "Training...\n",
      "----- Epoch 8 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.83125948905945.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.62579679489136.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 518.3026628494263.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 691.3767192363739.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 864.1748917102814.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoc h took: 877.5316967964172\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52031\n",
      "  F1: 0.64560\n",
      "  Validation Loss: 0.69988\n",
      "  Validation took: 47.06096529960632\n",
      "\n",
      "Training...\n",
      "----- Epoch 9 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.9756088256836.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.82127046585083.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 518.6471538543701.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 691.3623497486115.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 864.2934031486511.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoc h took: 877.6374578475952\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52031\n",
      "  F1: 0.64560\n",
      "  Validation Loss: 0.69988\n",
      "  Validation took: 47.06176543235779\n",
      "\n",
      "Training...\n",
      "----- Epoch 10 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.8135130405426.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.77911853790283.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 518.5905568599701.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 691.5588726997375.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 864.0565598011017.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoc h took: 877.4093887805939\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52031\n",
      "  F1: 0.64560\n",
      "  Validation Loss: 0.69988\n",
      "  Validation took: 47.04207420349121\n",
      "\n",
      "Training...\n",
      "----- Epoch 11 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.72724318504333.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.4826691150665.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 518.2807626724243.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 690.9948651790619.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 863.7250056266785.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoc h took: 877.0603959560394\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52031\n",
      "  F1: 0.64560\n",
      "  Validation Loss: 0.69988\n",
      "  Validation took: 47.03575539588928\n",
      "\n",
      "Training...\n",
      "----- Epoch 12 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.6593132019043.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.0245704650879.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 517.4983239173889.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 690.2469930648804.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 863.0051236152649.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoc h took: 876.2900123596191\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52031\n",
      "  F1: 0.64560\n",
      "  Validation Loss: 0.69988\n",
      "  Validation took: 47.049007177352905\n",
      "\n",
      "Training...\n",
      "----- Epoch 13 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.5469024181366.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.2360508441925.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 517.8424775600433.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 690.7908449172974.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 863.430299282074.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoc h took: 876.7809960842133\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52031\n",
      "  F1: 0.64560\n",
      "  Validation Loss: 0.69988\n",
      "  Validation took: 47.054142475128174\n",
      "\n",
      "Training...\n",
      "----- Epoch 14 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.55525827407837.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.346195936203.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 517.8121371269226.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 690.4900438785553.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 863.1699123382568.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoc h took: 876.5208706855774\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52031\n",
      "  F1: 0.64560\n",
      "  Validation Loss: 0.69988\n",
      "  Validation took: 47.076518058776855\n",
      "\n",
      "Training...\n",
      "----- Epoch 15 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.8533570766449.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.53643465042114.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 518.0092284679413.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 690.6511106491089.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 863.3058004379272.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoc h took: 876.6485280990601\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52031\n",
      "  F1: 0.64560\n",
      "  Validation Loss: 0.69988\n",
      "  Validation took: 47.06044960021973\n",
      "\n",
      "Training...\n",
      "----- Epoch 16 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.60780835151672.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.31804847717285.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 517.899409532547.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 690.4909288883209.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 863.2949779033661.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoc h took: 876.6294450759888\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.52031\n",
      "  F1: 0.64560\n",
      "  Validation Loss: 0.69988\n",
      "  Validation took: 47.05652165412903\n",
      "\n",
      "Training...\n",
      "----- Epoch 17 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.7119414806366.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.33397364616394.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 517.9527850151062.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 690.5747308731079.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 863.3465685844421.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoc h took: 876.7340185642242\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52031\n",
      "  F1: 0.64560\n",
      "  Validation Loss: 0.69988\n",
      "  Validation took: 47.070616245269775\n",
      "\n",
      "Training...\n",
      "----- Epoch 18 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.68400979042053.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.17225980758667.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 517.7609312534332.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 690.4170699119568.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 863.1185305118561.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoc h took: 876.485426902771\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52031\n",
      "  F1: 0.64560\n",
      "  Validation Loss: 0.69988\n",
      "  Validation took: 47.07424521446228\n",
      "\n",
      "Training...\n",
      "----- Epoch 19 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.5237922668457.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.0471029281616.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 518.0487580299377.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 690.8741042613983.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 863.5315263271332.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoc h took: 876.8537447452545\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52031\n",
      "  F1: 0.64560\n",
      "  Validation Loss: 0.69988\n",
      "  Validation took: 47.06574749946594\n",
      "\n",
      "Training...\n",
      "----- Epoch 20 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.71244025230408.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.37245416641235.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 517.9566762447357.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 690.4903151988983.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 863.1681823730469.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoc h took: 876.5131690502167\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52031\n",
      "  F1: 0.64560\n",
      "  Validation Loss: 0.69988\n",
      "  Validation took: 47.064488887786865\n"
     ]
    }
   ],
   "source": [
    "training_stats = []\n",
    "validations_labels_ep = []\n",
    "actual_labels_ep = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "for i in range(0, num_epochs):\n",
    "    print('')\n",
    "    print('Training...')\n",
    "    print('----- Epoch {:} / {:} -----'.format(i + 1, num_epochs))\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device).to(torch.int64)\n",
    "        b_input_mask = batch[1].to(device).to(torch.int64)\n",
    "        b_labels = batch[2].to(device).to(torch.int64)\n",
    "        \n",
    "        model.zero_grad()        \n",
    "\n",
    "        loss = model(b_input_ids, \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=b_input_mask,\n",
    "                     labels=b_labels)[0]\n",
    "        logits = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask,\n",
    "                       labels=b_labels)[1]\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    training_time = time.time() - t0\n",
    "\n",
    "    print('')\n",
    "    print('  Average training loss: {0:.2f}'.format(avg_train_loss))\n",
    "    print('  Training epoc h took: {:}'.format(training_time))\n",
    "    \n",
    "    print('')\n",
    "    print('Running Validation...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    total_eval_f1 = 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            loss = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)[0]\n",
    "\n",
    "            logits = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)[1]\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        \n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU:\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches:\n",
    "        \n",
    "        total_eval_accuracy += accuracy(logits, label_ids)\n",
    "        total_eval_f1 += flat_f1_score(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print('  Accuracy: {0:.5f}'.format(avg_val_accuracy))\n",
    "    \n",
    "    # Report the final f1 score for this validation run.\n",
    "    \n",
    "    avg_val_f1 = total_eval_f1 / len(validation_dataloader)\n",
    "    print('  F1: {0:.5f}'.format(avg_val_f1))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    \n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Measure how long the validation run took:\n",
    "    \n",
    "    validation_time = time.time() - t0\n",
    "    \n",
    "    print('  Validation Loss: {0:.5f}'.format(avg_val_loss))\n",
    "    print('  Validation took: {:}'.format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    \n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Val_F1' : avg_val_f1,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b180fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"./models/bert_{CUR_DATASET}_regexp_stopwords_{if_stopwords}_lemmatization_{if_lemmatize}_binary_SJ_2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp-transformers] *",
   "language": "python",
   "name": "conda-env-nlp-transformers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
