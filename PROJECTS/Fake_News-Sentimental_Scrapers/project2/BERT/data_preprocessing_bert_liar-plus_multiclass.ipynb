{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e31007",
   "metadata": {},
   "source": [
    "# Important Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ac1e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340b59f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a41d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")  # Punkt Sentence Tokenizer\n",
    "nltk.download(\"averaged_perceptron_tagger\")  # Part of Speech Tagger\n",
    "nltk.download(\"wordnet\")  # a lexical database of English; useful for synonyms, hyponyms, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f69fe551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1cbe4",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5205880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUR_DATASET = \"LIAR-PLUS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd79374",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(f'../data/{CUR_DATASET}/train2.tsv', sep='\\t', header = None)\n",
    "valid_dataset = pd.read_csv(f'../data/{CUR_DATASET}/val2.tsv', sep='\\t', header = None)\n",
    "test_dataset = pd.read_csv(f'../data/{CUR_DATASET}/test2.tsv', sep='\\t', header = None)\n",
    "liar_dataset = pd.concat([train_dataset, valid_dataset, test_dataset], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cb85e92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2635.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "      <td>That's a premise that he fails to back up. Ann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "      <td>Surovell said the decline of coal \"started whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>324.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "      <td>Obama said he would have voted against the ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1123.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>a news release</td>\n",
       "      <td>The release may have a point that Mikulskis co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>9028.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>an interview on CNN</td>\n",
       "      <td>Crist said that the economic \"turnaround start...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0           1            2   \\\n",
       "0  0.0   2635.json        false   \n",
       "1  1.0  10540.json    half-true   \n",
       "2  2.0    324.json  mostly-true   \n",
       "3  3.0   1123.json        false   \n",
       "4  4.0   9028.json    half-true   \n",
       "\n",
       "                                                  3   \\\n",
       "0  Says the Annies List political group supports ...   \n",
       "1  When did the decline of coal start? It started...   \n",
       "2  Hillary Clinton agrees with John McCain \"by vo...   \n",
       "3  Health care reform legislation is likely to ma...   \n",
       "4  The economic turnaround started at the end of ...   \n",
       "\n",
       "                                   4               5                     6   \\\n",
       "0                            abortion    dwayne-bohac  State representative   \n",
       "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
       "2                      foreign-policy    barack-obama             President   \n",
       "3                         health-care    blog-posting                   NaN   \n",
       "4                        economy,jobs   charlie-crist                   NaN   \n",
       "\n",
       "         7           8     9     10     11     12    13                   14  \\\n",
       "0     Texas  republican   0.0   1.0    0.0    0.0   0.0             a mailer   \n",
       "1  Virginia    democrat   0.0   0.0    1.0    1.0   0.0      a floor speech.   \n",
       "2  Illinois    democrat  70.0  71.0  160.0  163.0   9.0               Denver   \n",
       "3       NaN        none   7.0  19.0    3.0    5.0  44.0       a news release   \n",
       "4   Florida    democrat  15.0   9.0   20.0   19.0   2.0  an interview on CNN   \n",
       "\n",
       "                                                  15  \n",
       "0  That's a premise that he fails to back up. Ann...  \n",
       "1  Surovell said the decline of coal \"started whe...  \n",
       "2  Obama said he would have voted against the ame...  \n",
       "3  The release may have a point that Mikulskis co...  \n",
       "4  Crist said that the economic \"turnaround start...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liar_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0fec590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>statements</th>\n",
       "      <th>justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>That's a premise that he fails to back up. Ann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>Surovell said the decline of coal \"started whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>Obama said he would have voted against the ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>The release may have a point that Mikulskis co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>Crist said that the economic \"turnaround start...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                         statements  \\\n",
       "0        false  Says the Annies List political group supports ...   \n",
       "1    half-true  When did the decline of coal start? It started...   \n",
       "2  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
       "3        false  Health care reform legislation is likely to ma...   \n",
       "4    half-true  The economic turnaround started at the end of ...   \n",
       "\n",
       "                                       justification  \n",
       "0  That's a premise that he fails to back up. Ann...  \n",
       "1  Surovell said the decline of coal \"started whe...  \n",
       "2  Obama said he would have voted against the ame...  \n",
       "3  The release may have a point that Mikulskis co...  \n",
       "4  Crist said that the economic \"turnaround start...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liar_dataset = liar_dataset.iloc[:, [2, 3, 15]]\n",
    "liar_dataset = liar_dataset.rename(columns = {2: 'label', 3: 'statements', 15: 'justification'})\n",
    "liar_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9953dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_dataset['label'] = liar_dataset['label'].replace({\n",
    "    'false' : 0,\n",
    "    'barely-true' : 1,\n",
    "    'pants-fire' : 2,\n",
    "    'half-true' : 3,\n",
    "    'mostly-true' : 4,\n",
    "    'true' : 5\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f41a9f3",
   "metadata": {},
   "source": [
    "# Some More EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44d1480d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label              2\n",
       "statements         2\n",
       "justification    101\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liar_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20c24341",
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_dataset = liar_dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9302f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = liar_dataset[\"label\"].values.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011876e",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ee421",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe069c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4346120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_stopwords = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ddeb4e",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9bea26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from num2words import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f31cfa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_text_fn = {\n",
    "    \"no_punctuation\": lambda txt: re.sub(r'[^\\w\\s]','', txt),\n",
    "    \"no_special_symbols\": lambda txt: re.sub('[$,#,&]', '', txt),\n",
    "    # \"no_digits\": lambda txt: re.sub('\\d*', '', txt),\n",
    "    \"no_www\": lambda txt: re.sub('w{3}', '', txt),\n",
    "    \"no_urls\": lambda txt: re.sub('http\\S+', '', txt),\n",
    "    \"no_spaces\": lambda txt: re.sub('\\s+', ' ', txt),\n",
    "    \"no_single_chars\": lambda txt: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', txt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cd2a305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, pipeline = preprocessing_text_fn):\n",
    "    text = str(text)\n",
    "    for fn in pipeline.keys():\n",
    "        text = pipeline[fn](text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7276370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>statements</th>\n",
       "      <th>justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>Thats premise that he fails to back up Annies ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>When did the decline of coal start It started ...</td>\n",
       "      <td>Surovell said the decline of coal started when...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Hillary Clinton agrees with John McCain by vot...</td>\n",
       "      <td>Obama said he would have voted against the ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>The release may have point that Mikulskis comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>Crist said that the economic turnaround starte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>The Chicago Bears have had more starting quart...</td>\n",
       "      <td>But Vos specifically used the word fired which...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Jim Dunnam has not lived in the district he re...</td>\n",
       "      <td>But determining that would take significant de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Im the only person on this stage who has worke...</td>\n",
       "      <td>However it was not that bill but another one s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.0</td>\n",
       "      <td>However it took 195 million in Oregon Lottery ...</td>\n",
       "      <td>But Johnson is correct that many other factors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Says GOP primary opponents Glenn Grothman and ...</td>\n",
       "      <td>Considering that the 532 million figure covers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                         statements  \\\n",
       "0    0.0  Says the Annies List political group supports ...   \n",
       "1    3.0  When did the decline of coal start It started ...   \n",
       "2    4.0  Hillary Clinton agrees with John McCain by vot...   \n",
       "3    0.0  Health care reform legislation is likely to ma...   \n",
       "4    3.0  The economic turnaround started at the end of ...   \n",
       "5    5.0  The Chicago Bears have had more starting quart...   \n",
       "6    1.0  Jim Dunnam has not lived in the district he re...   \n",
       "7    3.0  Im the only person on this stage who has worke...   \n",
       "8    3.0  However it took 195 million in Oregon Lottery ...   \n",
       "9    4.0  Says GOP primary opponents Glenn Grothman and ...   \n",
       "\n",
       "                                       justification  \n",
       "0  Thats premise that he fails to back up Annies ...  \n",
       "1  Surovell said the decline of coal started when...  \n",
       "2  Obama said he would have voted against the ame...  \n",
       "3  The release may have point that Mikulskis comm...  \n",
       "4  Crist said that the economic turnaround starte...  \n",
       "5  But Vos specifically used the word fired which...  \n",
       "6  But determining that would take significant de...  \n",
       "7  However it was not that bill but another one s...  \n",
       "8  But Johnson is correct that many other factors...  \n",
       "9  Considering that the 532 million figure covers...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liar_dataset[\"statements\"] = liar_dataset[\"statements\"].apply(preprocess_text)\n",
    "liar_dataset[\"justification\"] = liar_dataset[\"justification\"].apply(preprocess_text)\n",
    "liar_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25112b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if if_stopwords:\n",
    "    for col in [\"statements\", \"justification\"]:\n",
    "        \n",
    "        liar_dataset[col] = liar_dataset[col].str.lower().str.replace(\"’\", \"'\")\n",
    "        liar_dataset[col] = liar_dataset[col].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6292eec",
   "metadata": {},
   "source": [
    "### Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4b4247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_lemmatize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e159b0b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "if if_lemmatize:\n",
    "    \n",
    "    import nltk\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    for col in [\"statements\", \"justification\"]:\n",
    "        liar_dataset[col] = liar_dataset[col].str.lower().str.replace(\"’\", \"'\")\n",
    "        liar_dataset[col] = liar_dataset[col].apply(lambda x: ' '.join([wnl.lemmatize(word) for word in word_tokenize(x)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "849ade27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = (liar_dataset[\"statements\"] + \" \" + liar_dataset[\"justification\"]).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e8bb8",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3eebdcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2a2ae17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marneusz/anaconda3/envs/nlp-transformers/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device('cuda')    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d41e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fada532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17dcf749",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7676622f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  say annies list political group support thirdtrimester abortion demand thats premise fails back annies list make bone comfortable candidate oppose restriction lateterm abortion year backing two house candidate voted limit\n",
      "Tokenized:  ['say', 'annie', '##s', 'list', 'political', 'group', 'support', 'third', '##tri', '##mes', '##ter', 'abortion', 'demand', 'that', '##s', 'premise', 'fails', 'back', 'annie', '##s', 'list', 'make', 'bone', 'comfortable', 'candidate', 'oppose', 'restriction', 'late', '##ter', '##m', 'abortion', 'year', 'backing', 'two', 'house', 'candidate', 'voted', 'limit']\n",
      "Token IDs:  [2360, 8194, 2015, 2862, 2576, 2177, 2490, 2353, 18886, 7834, 3334, 11324, 5157, 2008, 2015, 18458, 11896, 2067, 8194, 2015, 2862, 2191, 5923, 6625, 4018, 15391, 16840, 2397, 3334, 2213, 11324, 2095, 5150, 2048, 2160, 4018, 5444, 5787]\n"
     ]
    }
   ],
   "source": [
    "print(' Original: ', train_text[0])\n",
    "print('Tokenized: ', tokenizer.tokenize(train_text[0]))\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_text[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6a38f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', # Use the 124-layer, 1024-hidden, 16-heads, 340M parameters BERT model with an uncased vocab.\n",
    "    num_labels = len(np.unique(labels)), \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False, \n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1dce90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d8b1ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                             | 0/12692 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2089 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 12692/12692 [00:06<00:00, 2086.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  2089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "len_limit = 512\n",
    "LIMIT = 100_000\n",
    "\n",
    "indices = []\n",
    "train_text_filtered = []\n",
    "\n",
    "for i, text in enumerate(tqdm(train_text)):\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "    if len(input_ids) <= LIMIT:\n",
    "        train_text_filtered.append(text)\n",
    "        indices.append(i)\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94276bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12692,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_filtered = labels[indices]\n",
    "labels_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46221860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/jeongwonkim10516/nlp-fake-news-with-bert-99-55-top1/notebook\n",
    "\n",
    "def tokenize_map(sentence, labs='None'):\n",
    "    \n",
    "    \"\"\"A function for tokenize all of the sentences and map the tokens to their word IDs.\"\"\"\n",
    "    \n",
    "    global labels\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    \n",
    "    for text in tqdm(sentence):\n",
    "        #   \"encode_plus\" will:\n",
    "        \n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        \n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            text,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            truncation='longest_first', # Activate and control truncation\n",
    "                            max_length = len_limit,           # Max length according to our text data.\n",
    "                            padding = 'max_length', # Pad & truncate all sentences.\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the id list. \n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        \n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        \n",
    "    # Convert the lists into tensors.\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    if labs != 'None': # Setting this for using this definition for both train and test data so labels won't be a problem in our outputs.\n",
    "        labels = torch.tensor(labels)\n",
    "        return input_ids, attention_masks, labels\n",
    "    \n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae808334",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_filtered = np.array(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "deaf7201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12692,), (12692,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.shape, train_text_filtered.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd4cf520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 12692/12692 [00:07<00:00, 1761.63it/s]\n",
      "/tmp/ipykernel_2788/2816128611.py:47: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if labs != 'None': # Setting this for using this definition for both train and test data so labels won't be a problem in our outputs.\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_masks, labels_filtered = tokenize_map(train_text_filtered, labels_filtered)\n",
    "# test_input_ids, test_attention_masks= tokenize_map(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249a58e",
   "metadata": {},
   "source": [
    "## Train and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "745e5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34763ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0cf541b190>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 10\n",
    "transformers.set_seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d21142e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca257209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12692]), torch.Size([12692, 512]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_filtered.shape, input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c866b965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12692])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_filtered = labels[indices]\n",
    "labels_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c48fb067",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "30770dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_size, val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a427a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DataLoader\n",
    "batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  \n",
    "            sampler = RandomSampler(train_dataset), \n",
    "            batch_size = batch_size \n",
    "        )\n",
    "\n",
    "# Validation DataLoader\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            sampler = SequentialSampler(val_dataset), \n",
    "            batch_size = batch_size \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f3b8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test DataLoader\n",
    "\n",
    "# test_data = TensorDataset(test_input_ids, test_attention_masks)\n",
    "# test_sampler = SequentialSampler(test_data)\n",
    "# test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6152d",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "edf00b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5, # args.learning_rate\n",
    "                  eps = 1e-8 # args.adam_epsilon\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ee0f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "total_num_steps = len(train_dataloader) * num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae540af7",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c29ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    return accuracy_score(labels_flat, pred_flat)\n",
    "\n",
    "def flat_f1_score(predictions, labels):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "\n",
    "    return f1_score(labels_flat, pred_flat, zero_division=0, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf381a05",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d0f67393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e9f7c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "----- Epoch 1 / 10 -----\n",
      "  Batch    50  of  2,539.    Elapsed: 18.303362607955933.\n",
      "  Batch   100  of  2,539.    Elapsed: 35.51332187652588.\n",
      "  Batch   150  of  2,539.    Elapsed: 52.78431153297424.\n",
      "  Batch   200  of  2,539.    Elapsed: 69.9936306476593.\n",
      "  Batch   250  of  2,539.    Elapsed: 87.18361830711365.\n",
      "  Batch   300  of  2,539.    Elapsed: 104.59451794624329.\n",
      "  Batch   350  of  2,539.    Elapsed: 121.83549356460571.\n",
      "  Batch   400  of  2,539.    Elapsed: 139.08982133865356.\n",
      "  Batch   450  of  2,539.    Elapsed: 156.23701882362366.\n",
      "  Batch   500  of  2,539.    Elapsed: 173.56279182434082.\n",
      "  Batch   550  of  2,539.    Elapsed: 190.81710839271545.\n",
      "  Batch   600  of  2,539.    Elapsed: 208.09210538864136.\n",
      "  Batch   650  of  2,539.    Elapsed: 225.31372117996216.\n",
      "  Batch   700  of  2,539.    Elapsed: 242.76969575881958.\n",
      "  Batch   750  of  2,539.    Elapsed: 259.973751783371.\n",
      "  Batch   800  of  2,539.    Elapsed: 277.3064043521881.\n",
      "  Batch   850  of  2,539.    Elapsed: 294.6342988014221.\n",
      "  Batch   900  of  2,539.    Elapsed: 311.90012669563293.\n",
      "  Batch   950  of  2,539.    Elapsed: 329.1113088130951.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 346.52790665626526.\n",
      "  Batch 1,050  of  2,539.    Elapsed: 363.886593580246.\n",
      "  Batch 1,100  of  2,539.    Elapsed: 381.2813081741333.\n",
      "  Batch 1,150  of  2,539.    Elapsed: 398.6928198337555.\n",
      "  Batch 1,200  of  2,539.    Elapsed: 415.9823954105377.\n",
      "  Batch 1,250  of  2,539.    Elapsed: 433.2542576789856.\n",
      "  Batch 1,300  of  2,539.    Elapsed: 450.5978629589081.\n",
      "  Batch 1,350  of  2,539.    Elapsed: 467.9445004463196.\n",
      "  Batch 1,400  of  2,539.    Elapsed: 485.31896567344666.\n",
      "  Batch 1,450  of  2,539.    Elapsed: 502.70858359336853.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 520.1478743553162.\n",
      "  Batch 1,550  of  2,539.    Elapsed: 537.5545699596405.\n",
      "  Batch 1,600  of  2,539.    Elapsed: 554.9500041007996.\n",
      "  Batch 1,650  of  2,539.    Elapsed: 572.2848582267761.\n",
      "  Batch 1,700  of  2,539.    Elapsed: 589.5448956489563.\n",
      "  Batch 1,750  of  2,539.    Elapsed: 606.8293323516846.\n",
      "  Batch 1,800  of  2,539.    Elapsed: 624.1421358585358.\n",
      "  Batch 1,850  of  2,539.    Elapsed: 641.5656378269196.\n",
      "  Batch 1,900  of  2,539.    Elapsed: 658.8860056400299.\n",
      "  Batch 1,950  of  2,539.    Elapsed: 676.1266386508942.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 693.406298160553.\n",
      "  Batch 2,050  of  2,539.    Elapsed: 710.6361961364746.\n",
      "  Batch 2,100  of  2,539.    Elapsed: 727.942058801651.\n",
      "  Batch 2,150  of  2,539.    Elapsed: 745.2516570091248.\n",
      "  Batch 2,200  of  2,539.    Elapsed: 762.5331466197968.\n",
      "  Batch 2,250  of  2,539.    Elapsed: 779.7715854644775.\n",
      "  Batch 2,300  of  2,539.    Elapsed: 797.1174147129059.\n",
      "  Batch 2,350  of  2,539.    Elapsed: 814.4500648975372.\n",
      "  Batch 2,400  of  2,539.    Elapsed: 831.6442685127258.\n",
      "  Batch 2,450  of  2,539.    Elapsed: 848.8773391246796.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 866.1661913394928.\n",
      "\n",
      "  Average training loss: 1.75\n",
      "  Training epoc h took: 879.4107010364532\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.23202\n",
      "  F1: 0.19390\n",
      "  Validation Loss: 1.74152\n",
      "  Validation took: 93.31734561920166\n",
      "\n",
      "Training...\n",
      "----- Epoch 2 / 10 -----\n",
      "  Batch    50  of  2,539.    Elapsed: 17.274221897125244.\n",
      "  Batch   100  of  2,539.    Elapsed: 34.539084911346436.\n",
      "  Batch   150  of  2,539.    Elapsed: 51.81147909164429.\n",
      "  Batch   200  of  2,539.    Elapsed: 69.06715202331543.\n",
      "  Batch   250  of  2,539.    Elapsed: 86.28868889808655.\n",
      "  Batch   300  of  2,539.    Elapsed: 103.54531145095825.\n",
      "  Batch   350  of  2,539.    Elapsed: 120.82129549980164.\n",
      "  Batch   400  of  2,539.    Elapsed: 138.12674736976624.\n",
      "  Batch   450  of  2,539.    Elapsed: 155.37146186828613.\n",
      "  Batch   500  of  2,539.    Elapsed: 172.53953170776367.\n",
      "  Batch   550  of  2,539.    Elapsed: 189.7495255470276.\n",
      "  Batch   600  of  2,539.    Elapsed: 207.00471568107605.\n",
      "  Batch   650  of  2,539.    Elapsed: 224.27013421058655.\n",
      "  Batch   700  of  2,539.    Elapsed: 241.66705417633057.\n",
      "  Batch   750  of  2,539.    Elapsed: 259.05919885635376.\n",
      "  Batch   800  of  2,539.    Elapsed: 276.69538831710815.\n",
      "  Batch   850  of  2,539.    Elapsed: 294.2752323150635.\n",
      "  Batch   900  of  2,539.    Elapsed: 311.4802756309509.\n",
      "  Batch   950  of  2,539.    Elapsed: 328.66040086746216.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.8600583076477.\n",
      "  Batch 1,050  of  2,539.    Elapsed: 363.02449798583984.\n",
      "  Batch 1,100  of  2,539.    Elapsed: 380.2640664577484.\n",
      "  Batch 1,150  of  2,539.    Elapsed: 397.40304684638977.\n",
      "  Batch 1,200  of  2,539.    Elapsed: 414.61508202552795.\n",
      "  Batch 1,250  of  2,539.    Elapsed: 431.81008434295654.\n",
      "  Batch 1,300  of  2,539.    Elapsed: 448.99778723716736.\n",
      "  Batch 1,350  of  2,539.    Elapsed: 466.21580958366394.\n",
      "  Batch 1,400  of  2,539.    Elapsed: 483.4239320755005.\n",
      "  Batch 1,450  of  2,539.    Elapsed: 500.6431224346161.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 517.8137221336365.\n",
      "  Batch 1,550  of  2,539.    Elapsed: 534.97536444664.\n",
      "  Batch 1,600  of  2,539.    Elapsed: 552.1159296035767.\n",
      "  Batch 1,650  of  2,539.    Elapsed: 569.2902631759644.\n",
      "  Batch 1,700  of  2,539.    Elapsed: 586.4365320205688.\n",
      "  Batch 1,750  of  2,539.    Elapsed: 603.6890275478363.\n",
      "  Batch 1,800  of  2,539.    Elapsed: 621.0455708503723.\n",
      "  Batch 1,850  of  2,539.    Elapsed: 638.7166030406952.\n",
      "  Batch 1,900  of  2,539.    Elapsed: 657.7989830970764.\n",
      "  Batch 1,950  of  2,539.    Elapsed: 675.8889391422272.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 694.13640832901.\n",
      "  Batch 2,050  of  2,539.    Elapsed: 712.7321903705597.\n",
      "  Batch 2,100  of  2,539.    Elapsed: 730.0867018699646.\n",
      "  Batch 2,150  of  2,539.    Elapsed: 747.8435728549957.\n",
      "  Batch 2,200  of  2,539.    Elapsed: 765.3398284912109.\n",
      "  Batch 2,250  of  2,539.    Elapsed: 783.2588920593262.\n",
      "  Batch 2,300  of  2,539.    Elapsed: 801.5427687168121.\n",
      "  Batch 2,350  of  2,539.    Elapsed: 819.6555919647217.\n",
      "  Batch 2,400  of  2,539.    Elapsed: 837.3742251396179.\n",
      "  Batch 2,450  of  2,539.    Elapsed: 855.0151836872101.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 873.0393469333649.\n",
      "\n",
      "  Average training loss: 1.69\n",
      "  Training epoc h took: 887.0245928764343\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.25367\n",
      "  F1: 0.24005\n",
      "  Validation Loss: 1.72611\n",
      "  Validation took: 97.59020233154297\n",
      "\n",
      "Training...\n",
      "----- Epoch 3 / 10 -----\n",
      "  Batch    50  of  2,539.    Elapsed: 18.157809019088745.\n",
      "  Batch   100  of  2,539.    Elapsed: 36.92732810974121.\n",
      "  Batch   150  of  2,539.    Elapsed: 54.85864996910095.\n",
      "  Batch   200  of  2,539.    Elapsed: 73.02128481864929.\n",
      "  Batch   250  of  2,539.    Elapsed: 91.65829944610596.\n",
      "  Batch   300  of  2,539.    Elapsed: 109.72180986404419.\n",
      "  Batch   350  of  2,539.    Elapsed: 127.8560152053833.\n",
      "  Batch   400  of  2,539.    Elapsed: 146.57698035240173.\n",
      "  Batch   450  of  2,539.    Elapsed: 165.2677812576294.\n",
      "  Batch   500  of  2,539.    Elapsed: 183.90604257583618.\n",
      "  Batch   550  of  2,539.    Elapsed: 203.06698036193848.\n",
      "  Batch   600  of  2,539.    Elapsed: 222.0326828956604.\n",
      "  Batch   650  of  2,539.    Elapsed: 240.23260831832886.\n",
      "  Batch   700  of  2,539.    Elapsed: 257.8234052658081.\n",
      "  Batch   750  of  2,539.    Elapsed: 275.0541443824768.\n",
      "  Batch   800  of  2,539.    Elapsed: 292.38066387176514.\n",
      "  Batch   850  of  2,539.    Elapsed: 310.3372550010681.\n",
      "  Batch   900  of  2,539.    Elapsed: 328.5302429199219.\n",
      "  Batch   950  of  2,539.    Elapsed: 346.6049165725708.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 364.7616217136383.\n",
      "  Batch 1,050  of  2,539.    Elapsed: 382.88363313674927.\n",
      "  Batch 1,100  of  2,539.    Elapsed: 400.4519057273865.\n",
      "  Batch 1,150  of  2,539.    Elapsed: 418.4033668041229.\n",
      "  Batch 1,200  of  2,539.    Elapsed: 436.50345277786255.\n",
      "  Batch 1,250  of  2,539.    Elapsed: 454.3204519748688.\n",
      "  Batch 1,300  of  2,539.    Elapsed: 471.73274278640747.\n",
      "  Batch 1,350  of  2,539.    Elapsed: 488.9691457748413.\n",
      "  Batch 1,400  of  2,539.    Elapsed: 506.21698117256165.\n",
      "  Batch 1,450  of  2,539.    Elapsed: 523.5381360054016.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 540.7746012210846.\n",
      "  Batch 1,550  of  2,539.    Elapsed: 558.0426723957062.\n",
      "  Batch 1,600  of  2,539.    Elapsed: 575.2625615596771.\n",
      "  Batch 1,650  of  2,539.    Elapsed: 592.4394097328186.\n",
      "  Batch 1,700  of  2,539.    Elapsed: 610.1919128894806.\n",
      "  Batch 1,750  of  2,539.    Elapsed: 628.0695872306824.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,800  of  2,539.    Elapsed: 645.7612128257751.\n",
      "  Batch 1,850  of  2,539.    Elapsed: 663.8606269359589.\n",
      "  Batch 1,900  of  2,539.    Elapsed: 681.8338978290558.\n",
      "  Batch 1,950  of  2,539.    Elapsed: 699.8921194076538.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 718.2128450870514.\n",
      "  Batch 2,050  of  2,539.    Elapsed: 735.9833950996399.\n",
      "  Batch 2,100  of  2,539.    Elapsed: 753.4813792705536.\n",
      "  Batch 2,150  of  2,539.    Elapsed: 771.1213960647583.\n",
      "  Batch 2,200  of  2,539.    Elapsed: 789.130690574646.\n",
      "  Batch 2,250  of  2,539.    Elapsed: 807.2025887966156.\n",
      "  Batch 2,300  of  2,539.    Elapsed: 825.3646187782288.\n",
      "  Batch 2,350  of  2,539.    Elapsed: 842.8635346889496.\n",
      "  Batch 2,400  of  2,539.    Elapsed: 861.0281116962433.\n",
      "  Batch 2,450  of  2,539.    Elapsed: 879.165766954422.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 897.3422610759735.\n",
      "\n",
      "  Average training loss: 1.54\n",
      "  Training epoc h took: 911.0959737300873\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.25315\n",
      "  F1: 0.24778\n",
      "  Validation Loss: 1.79603\n",
      "  Validation took: 97.34973406791687\n",
      "\n",
      "Training...\n",
      "----- Epoch 4 / 10 -----\n",
      "  Batch    50  of  2,539.    Elapsed: 17.82189178466797.\n",
      "  Batch   100  of  2,539.    Elapsed: 35.29279112815857.\n",
      "  Batch   150  of  2,539.    Elapsed: 52.536585092544556.\n",
      "  Batch   200  of  2,539.    Elapsed: 69.73172903060913.\n",
      "  Batch   250  of  2,539.    Elapsed: 86.9528419971466.\n",
      "  Batch   300  of  2,539.    Elapsed: 104.13228988647461.\n",
      "  Batch   350  of  2,539.    Elapsed: 121.35307836532593.\n",
      "  Batch   400  of  2,539.    Elapsed: 138.57729244232178.\n",
      "  Batch   450  of  2,539.    Elapsed: 155.8532898426056.\n",
      "  Batch   500  of  2,539.    Elapsed: 173.070472240448.\n",
      "  Batch   550  of  2,539.    Elapsed: 190.47534346580505.\n",
      "  Batch   600  of  2,539.    Elapsed: 208.54096269607544.\n",
      "  Batch   650  of  2,539.    Elapsed: 226.66477823257446.\n",
      "  Batch   700  of  2,539.    Elapsed: 244.38452315330505.\n",
      "  Batch   750  of  2,539.    Elapsed: 261.62236189842224.\n",
      "  Batch   800  of  2,539.    Elapsed: 278.8814573287964.\n",
      "  Batch   850  of  2,539.    Elapsed: 296.03431129455566.\n",
      "  Batch   900  of  2,539.    Elapsed: 314.0630033016205.\n",
      "  Batch   950  of  2,539.    Elapsed: 332.077401638031.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 350.12692856788635.\n",
      "  Batch 1,050  of  2,539.    Elapsed: 368.08436155319214.\n",
      "  Batch 1,100  of  2,539.    Elapsed: 386.10514521598816.\n",
      "  Batch 1,150  of  2,539.    Elapsed: 404.3232972621918.\n",
      "  Batch 1,200  of  2,539.    Elapsed: 422.0181713104248.\n",
      "  Batch 1,250  of  2,539.    Elapsed: 440.5012891292572.\n",
      "  Batch 1,300  of  2,539.    Elapsed: 458.7364354133606.\n",
      "  Batch 1,350  of  2,539.    Elapsed: 476.1422245502472.\n",
      "  Batch 1,400  of  2,539.    Elapsed: 493.75841760635376.\n",
      "  Batch 1,450  of  2,539.    Elapsed: 512.298930644989.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 529.631320476532.\n",
      "  Batch 1,550  of  2,539.    Elapsed: 547.4886372089386.\n",
      "  Batch 1,600  of  2,539.    Elapsed: 565.473265171051.\n",
      "  Batch 1,650  of  2,539.    Elapsed: 583.5870490074158.\n",
      "  Batch 1,700  of  2,539.    Elapsed: 600.8429465293884.\n",
      "  Batch 1,750  of  2,539.    Elapsed: 618.0442111492157.\n",
      "  Batch 1,800  of  2,539.    Elapsed: 635.3291327953339.\n",
      "  Batch 1,850  of  2,539.    Elapsed: 652.5099568367004.\n",
      "  Batch 1,900  of  2,539.    Elapsed: 670.0007789134979.\n",
      "  Batch 1,950  of  2,539.    Elapsed: 688.004367351532.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 705.7789697647095.\n",
      "  Batch 2,050  of  2,539.    Elapsed: 723.2820568084717.\n",
      "  Batch 2,100  of  2,539.    Elapsed: 740.8471493721008.\n",
      "  Batch 2,150  of  2,539.    Elapsed: 758.440000295639.\n",
      "  Batch 2,200  of  2,539.    Elapsed: 776.1547522544861.\n",
      "  Batch 2,250  of  2,539.    Elapsed: 794.6301839351654.\n",
      "  Batch 2,300  of  2,539.    Elapsed: 812.7978940010071.\n",
      "  Batch 2,350  of  2,539.    Elapsed: 831.1756417751312.\n",
      "  Batch 2,400  of  2,539.    Elapsed: 849.2825071811676.\n",
      "  Batch 2,450  of  2,539.    Elapsed: 867.5972163677216.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 885.661637544632.\n",
      "\n",
      "  Average training loss: 1.29\n",
      "  Training epoc h took: 899.4284510612488\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.24409\n",
      "  F1: 0.23587\n",
      "  Validation Loss: 2.05279\n",
      "  Validation took: 96.4283926486969\n",
      "\n",
      "Training...\n",
      "----- Epoch 5 / 10 -----\n",
      "  Batch    50  of  2,539.    Elapsed: 17.793876886367798.\n",
      "  Batch   100  of  2,539.    Elapsed: 35.17844605445862.\n",
      "  Batch   150  of  2,539.    Elapsed: 52.333964586257935.\n",
      "  Batch   200  of  2,539.    Elapsed: 69.56389427185059.\n",
      "  Batch   250  of  2,539.    Elapsed: 86.80767750740051.\n",
      "  Batch   300  of  2,539.    Elapsed: 104.20932912826538.\n",
      "  Batch   350  of  2,539.    Elapsed: 122.06045770645142.\n",
      "  Batch   400  of  2,539.    Elapsed: 139.3125126361847.\n",
      "  Batch   450  of  2,539.    Elapsed: 156.94363641738892.\n",
      "  Batch   500  of  2,539.    Elapsed: 174.79962992668152.\n",
      "  Batch   550  of  2,539.    Elapsed: 192.6811866760254.\n",
      "  Batch   600  of  2,539.    Elapsed: 210.59381818771362.\n",
      "  Batch   650  of  2,539.    Elapsed: 227.75184893608093.\n",
      "  Batch   700  of  2,539.    Elapsed: 245.61517024040222.\n",
      "  Batch   750  of  2,539.    Elapsed: 263.4276168346405.\n",
      "  Batch   800  of  2,539.    Elapsed: 281.2142677307129.\n",
      "  Batch   850  of  2,539.    Elapsed: 299.1788127422333.\n",
      "  Batch   900  of  2,539.    Elapsed: 316.7842969894409.\n",
      "  Batch   950  of  2,539.    Elapsed: 334.7502670288086.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 352.83183670043945.\n",
      "  Batch 1,050  of  2,539.    Elapsed: 371.06458616256714.\n",
      "  Batch 1,100  of  2,539.    Elapsed: 389.22625374794006.\n",
      "  Batch 1,150  of  2,539.    Elapsed: 407.0728511810303.\n",
      "  Batch 1,200  of  2,539.    Elapsed: 424.50467705726624.\n",
      "  Batch 1,250  of  2,539.    Elapsed: 441.9449677467346.\n",
      "  Batch 1,300  of  2,539.    Elapsed: 459.5202827453613.\n",
      "  Batch 1,350  of  2,539.    Elapsed: 477.01849269866943.\n",
      "  Batch 1,400  of  2,539.    Elapsed: 494.2147636413574.\n",
      "  Batch 1,450  of  2,539.    Elapsed: 511.7052664756775.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 529.9291009902954.\n",
      "  Batch 1,550  of  2,539.    Elapsed: 547.5086855888367.\n",
      "  Batch 1,600  of  2,539.    Elapsed: 565.4141488075256.\n",
      "  Batch 1,650  of  2,539.    Elapsed: 583.7208979129791.\n",
      "  Batch 1,700  of  2,539.    Elapsed: 601.3723864555359.\n",
      "  Batch 1,750  of  2,539.    Elapsed: 619.0818390846252.\n",
      "  Batch 1,800  of  2,539.    Elapsed: 637.1333653926849.\n",
      "  Batch 1,850  of  2,539.    Elapsed: 654.8968915939331.\n",
      "  Batch 1,900  of  2,539.    Elapsed: 672.5607469081879.\n",
      "  Batch 1,950  of  2,539.    Elapsed: 689.8767623901367.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 707.1831483840942.\n",
      "  Batch 2,050  of  2,539.    Elapsed: 724.4547278881073.\n",
      "  Batch 2,100  of  2,539.    Elapsed: 741.8999354839325.\n",
      "  Batch 2,150  of  2,539.    Elapsed: 759.3604626655579.\n",
      "  Batch 2,200  of  2,539.    Elapsed: 777.0303411483765.\n",
      "  Batch 2,250  of  2,539.    Elapsed: 794.751932144165.\n",
      "  Batch 2,300  of  2,539.    Elapsed: 812.8964765071869.\n",
      "  Batch 2,350  of  2,539.    Elapsed: 831.2151188850403.\n",
      "  Batch 2,400  of  2,539.    Elapsed: 849.0084345340729.\n",
      "  Batch 2,450  of  2,539.    Elapsed: 866.9028658866882.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 884.9836905002594.\n",
      "\n",
      "  Average training loss: 0.99\n",
      "  Training epoc h took: 898.9565815925598\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.25315\n",
      "  F1: 0.24881\n",
      "  Validation Loss: 2.36120\n",
      "  Validation took: 95.90858578681946\n",
      "\n",
      "Training...\n",
      "----- Epoch 6 / 10 -----\n",
      "  Batch    50  of  2,539.    Elapsed: 17.83452558517456.\n",
      "  Batch   100  of  2,539.    Elapsed: 35.27867913246155.\n",
      "  Batch   150  of  2,539.    Elapsed: 52.51931285858154.\n",
      "  Batch   200  of  2,539.    Elapsed: 70.40304589271545.\n",
      "  Batch   250  of  2,539.    Elapsed: 88.98688244819641.\n",
      "  Batch   300  of  2,539.    Elapsed: 107.28192615509033.\n",
      "  Batch   350  of  2,539.    Elapsed: 125.29672694206238.\n",
      "  Batch   400  of  2,539.    Elapsed: 143.24475932121277.\n",
      "  Batch   450  of  2,539.    Elapsed: 161.15402030944824.\n",
      "  Batch   500  of  2,539.    Elapsed: 178.6655135154724.\n",
      "  Batch   550  of  2,539.    Elapsed: 196.63480138778687.\n",
      "  Batch   600  of  2,539.    Elapsed: 215.04621529579163.\n",
      "  Batch   650  of  2,539.    Elapsed: 232.22351121902466.\n",
      "  Batch   700  of  2,539.    Elapsed: 249.82122254371643.\n",
      "  Batch   750  of  2,539.    Elapsed: 268.1108703613281.\n",
      "  Batch   800  of  2,539.    Elapsed: 286.32707166671753.\n",
      "  Batch   850  of  2,539.    Elapsed: 303.94849038124084.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   900  of  2,539.    Elapsed: 321.51792788505554.\n",
      "  Batch   950  of  2,539.    Elapsed: 339.1086919307709.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 356.4117329120636.\n",
      "  Batch 1,050  of  2,539.    Elapsed: 373.70496821403503.\n",
      "  Batch 1,100  of  2,539.    Elapsed: 391.0065960884094.\n",
      "  Batch 1,150  of  2,539.    Elapsed: 408.32973551750183.\n",
      "  Batch 1,200  of  2,539.    Elapsed: 425.715389251709.\n",
      "  Batch 1,250  of  2,539.    Elapsed: 442.95725321769714.\n",
      "  Batch 1,300  of  2,539.    Elapsed: 460.19599175453186.\n",
      "  Batch 1,350  of  2,539.    Elapsed: 477.50201296806335.\n",
      "  Batch 1,400  of  2,539.    Elapsed: 494.73108196258545.\n",
      "  Batch 1,450  of  2,539.    Elapsed: 511.973566532135.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 529.2863345146179.\n",
      "  Batch 1,550  of  2,539.    Elapsed: 546.6035168170929.\n",
      "  Batch 1,600  of  2,539.    Elapsed: 563.9325993061066.\n",
      "  Batch 1,650  of  2,539.    Elapsed: 581.1785669326782.\n",
      "  Batch 1,700  of  2,539.    Elapsed: 598.4207322597504.\n",
      "  Batch 1,750  of  2,539.    Elapsed: 615.6920137405396.\n",
      "  Batch 1,800  of  2,539.    Elapsed: 632.8757119178772.\n",
      "  Batch 1,850  of  2,539.    Elapsed: 650.1160297393799.\n",
      "  Batch 1,900  of  2,539.    Elapsed: 667.322292804718.\n",
      "  Batch 1,950  of  2,539.    Elapsed: 684.5149323940277.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 701.7538325786591.\n",
      "  Batch 2,050  of  2,539.    Elapsed: 718.981448173523.\n",
      "  Batch 2,100  of  2,539.    Elapsed: 737.1329581737518.\n",
      "  Batch 2,150  of  2,539.    Elapsed: 755.5233874320984.\n",
      "  Batch 2,200  of  2,539.    Elapsed: 773.9455885887146.\n",
      "  Batch 2,250  of  2,539.    Elapsed: 791.7159314155579.\n",
      "  Batch 2,300  of  2,539.    Elapsed: 809.6719491481781.\n",
      "  Batch 2,350  of  2,539.    Elapsed: 827.214103937149.\n",
      "  Batch 2,400  of  2,539.    Elapsed: 844.6945641040802.\n",
      "  Batch 2,450  of  2,539.    Elapsed: 862.1619958877563.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 879.7989542484283.\n",
      "\n",
      "  Average training loss: 0.73\n",
      "  Training epoc h took: 893.2465713024139\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.25079\n",
      "  F1: 0.23381\n",
      "  Validation Loss: 3.05544\n",
      "  Validation took: 94.41857647895813\n",
      "\n",
      "Training...\n",
      "----- Epoch 7 / 10 -----\n",
      "  Batch    50  of  2,539.    Elapsed: 17.48392963409424.\n",
      "  Batch   100  of  2,539.    Elapsed: 34.966461420059204.\n",
      "  Batch   150  of  2,539.    Elapsed: 52.389801263809204.\n",
      "  Batch   200  of  2,539.    Elapsed: 70.0557119846344.\n",
      "  Batch   250  of  2,539.    Elapsed: 87.51882910728455.\n",
      "  Batch   300  of  2,539.    Elapsed: 104.98493385314941.\n",
      "  Batch   350  of  2,539.    Elapsed: 122.34833884239197.\n",
      "  Batch   400  of  2,539.    Elapsed: 139.88945412635803.\n",
      "  Batch   450  of  2,539.    Elapsed: 157.3236951828003.\n",
      "  Batch   500  of  2,539.    Elapsed: 174.8528962135315.\n",
      "  Batch   550  of  2,539.    Elapsed: 192.43689489364624.\n",
      "  Batch   600  of  2,539.    Elapsed: 209.98128747940063.\n",
      "  Batch   650  of  2,539.    Elapsed: 227.35259127616882.\n",
      "  Batch   700  of  2,539.    Elapsed: 244.64174127578735.\n",
      "  Batch   750  of  2,539.    Elapsed: 261.98398876190186.\n",
      "  Batch   800  of  2,539.    Elapsed: 279.3604156970978.\n",
      "  Batch   850  of  2,539.    Elapsed: 296.5737843513489.\n",
      "  Batch   900  of  2,539.    Elapsed: 313.8947958946228.\n",
      "  Batch   950  of  2,539.    Elapsed: 331.2297582626343.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 348.5293037891388.\n",
      "  Batch 1,050  of  2,539.    Elapsed: 365.7525110244751.\n",
      "  Batch 1,100  of  2,539.    Elapsed: 382.95456099510193.\n",
      "  Batch 1,150  of  2,539.    Elapsed: 400.1950271129608.\n",
      "  Batch 1,200  of  2,539.    Elapsed: 417.5015733242035.\n",
      "  Batch 1,250  of  2,539.    Elapsed: 434.7428846359253.\n",
      "  Batch 1,300  of  2,539.    Elapsed: 452.05283427238464.\n",
      "  Batch 1,350  of  2,539.    Elapsed: 469.30998945236206.\n",
      "  Batch 1,400  of  2,539.    Elapsed: 486.62092089653015.\n",
      "  Batch 1,450  of  2,539.    Elapsed: 503.87736797332764.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 521.2010436058044.\n",
      "  Batch 1,550  of  2,539.    Elapsed: 538.5199990272522.\n",
      "  Batch 1,600  of  2,539.    Elapsed: 555.7673625946045.\n",
      "  Batch 1,650  of  2,539.    Elapsed: 573.1806402206421.\n",
      "  Batch 1,700  of  2,539.    Elapsed: 590.4914617538452.\n",
      "  Batch 1,750  of  2,539.    Elapsed: 607.720342874527.\n",
      "  Batch 1,800  of  2,539.    Elapsed: 624.9477205276489.\n",
      "  Batch 1,850  of  2,539.    Elapsed: 642.39537525177.\n",
      "  Batch 1,900  of  2,539.    Elapsed: 659.7827847003937.\n",
      "  Batch 1,950  of  2,539.    Elapsed: 677.0302050113678.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 694.4701647758484.\n",
      "  Batch 2,050  of  2,539.    Elapsed: 712.0145456790924.\n",
      "  Batch 2,100  of  2,539.    Elapsed: 729.5982286930084.\n",
      "  Batch 2,150  of  2,539.    Elapsed: 747.2457859516144.\n",
      "  Batch 2,200  of  2,539.    Elapsed: 764.8537485599518.\n",
      "  Batch 2,250  of  2,539.    Elapsed: 782.3995172977448.\n",
      "  Batch 2,300  of  2,539.    Elapsed: 799.9597814083099.\n",
      "  Batch 2,350  of  2,539.    Elapsed: 817.5923402309418.\n",
      "  Batch 2,400  of  2,539.    Elapsed: 835.1502840518951.\n",
      "  Batch 2,450  of  2,539.    Elapsed: 852.7983989715576.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 870.3701815605164.\n",
      "\n",
      "  Average training loss: 0.52\n",
      "  Training epoc h took: 883.94384932518\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.24094\n",
      "  F1: 0.22878\n",
      "  Validation Loss: 4.20281\n",
      "  Validation took: 92.59905123710632\n",
      "\n",
      "Training...\n",
      "----- Epoch 8 / 10 -----\n",
      "  Batch    50  of  2,539.    Elapsed: 17.239540576934814.\n",
      "  Batch   100  of  2,539.    Elapsed: 34.48393797874451.\n",
      "  Batch   150  of  2,539.    Elapsed: 51.83567190170288.\n",
      "  Batch   200  of  2,539.    Elapsed: 69.05586171150208.\n",
      "  Batch   250  of  2,539.    Elapsed: 86.39644813537598.\n",
      "  Batch   300  of  2,539.    Elapsed: 103.64828443527222.\n",
      "  Batch   350  of  2,539.    Elapsed: 121.09668850898743.\n",
      "  Batch   400  of  2,539.    Elapsed: 138.386372089386.\n",
      "  Batch   450  of  2,539.    Elapsed: 155.56414699554443.\n",
      "  Batch   500  of  2,539.    Elapsed: 172.75318956375122.\n",
      "  Batch   550  of  2,539.    Elapsed: 190.0905921459198.\n",
      "  Batch   600  of  2,539.    Elapsed: 207.38430976867676.\n",
      "  Batch   650  of  2,539.    Elapsed: 224.71817994117737.\n",
      "  Batch   700  of  2,539.    Elapsed: 242.0539469718933.\n",
      "  Batch   750  of  2,539.    Elapsed: 259.29119205474854.\n",
      "  Batch   800  of  2,539.    Elapsed: 276.65388774871826.\n",
      "  Batch   850  of  2,539.    Elapsed: 293.9392385482788.\n",
      "  Batch   900  of  2,539.    Elapsed: 311.2229368686676.\n",
      "  Batch   950  of  2,539.    Elapsed: 328.5683286190033.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 346.02399373054504.\n",
      "  Batch 1,050  of  2,539.    Elapsed: 363.55760169029236.\n",
      "  Batch 1,100  of  2,539.    Elapsed: 381.0810477733612.\n",
      "  Batch 1,150  of  2,539.    Elapsed: 398.5678942203522.\n",
      "  Batch 1,200  of  2,539.    Elapsed: 415.9942123889923.\n",
      "  Batch 1,250  of  2,539.    Elapsed: 433.40892720222473.\n",
      "  Batch 1,300  of  2,539.    Elapsed: 450.878214597702.\n",
      "  Batch 1,350  of  2,539.    Elapsed: 468.4525034427643.\n",
      "  Batch 1,400  of  2,539.    Elapsed: 485.8878004550934.\n",
      "  Batch 1,450  of  2,539.    Elapsed: 503.3687045574188.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 520.7884261608124.\n",
      "  Batch 1,550  of  2,539.    Elapsed: 538.2645366191864.\n",
      "  Batch 1,600  of  2,539.    Elapsed: 555.7131948471069.\n",
      "  Batch 1,650  of  2,539.    Elapsed: 573.1199927330017.\n",
      "  Batch 1,700  of  2,539.    Elapsed: 590.5678935050964.\n",
      "  Batch 1,750  of  2,539.    Elapsed: 608.0113134384155.\n",
      "  Batch 1,800  of  2,539.    Elapsed: 625.4043624401093.\n",
      "  Batch 1,850  of  2,539.    Elapsed: 642.8778376579285.\n",
      "  Batch 1,900  of  2,539.    Elapsed: 660.4289767742157.\n",
      "  Batch 1,950  of  2,539.    Elapsed: 678.0051832199097.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 695.5070867538452.\n",
      "  Batch 2,050  of  2,539.    Elapsed: 713.1868915557861.\n",
      "  Batch 2,100  of  2,539.    Elapsed: 730.6584775447845.\n",
      "  Batch 2,150  of  2,539.    Elapsed: 748.1419186592102.\n",
      "  Batch 2,200  of  2,539.    Elapsed: 765.6622157096863.\n",
      "  Batch 2,250  of  2,539.    Elapsed: 783.1346688270569.\n",
      "  Batch 2,300  of  2,539.    Elapsed: 800.5123007297516.\n",
      "  Batch 2,350  of  2,539.    Elapsed: 817.9811642169952.\n",
      "  Batch 2,400  of  2,539.    Elapsed: 835.4379849433899.\n",
      "  Batch 2,450  of  2,539.    Elapsed: 853.0137934684753.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 870.5418291091919.\n",
      "\n",
      "  Average training loss: 0.45\n",
      "  Training epoc h took: 883.9127991199493\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.23150\n",
      "  F1: 0.22645\n",
      "  Validation Loss: 5.19958\n",
      "  Validation took: 94.0017602443695\n",
      "\n",
      "Training...\n",
      "----- Epoch 9 / 10 -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    50  of  2,539.    Elapsed: 17.44432258605957.\n",
      "  Batch   100  of  2,539.    Elapsed: 35.032318115234375.\n",
      "  Batch   150  of  2,539.    Elapsed: 52.51203465461731.\n",
      "  Batch   200  of  2,539.    Elapsed: 70.08521485328674.\n",
      "  Batch   250  of  2,539.    Elapsed: 87.59621214866638.\n",
      "  Batch   300  of  2,539.    Elapsed: 105.18247103691101.\n",
      "  Batch   350  of  2,539.    Elapsed: 122.68001341819763.\n",
      "  Batch   400  of  2,539.    Elapsed: 140.1168806552887.\n",
      "  Batch   450  of  2,539.    Elapsed: 157.35289406776428.\n",
      "  Batch   500  of  2,539.    Elapsed: 174.85365056991577.\n",
      "  Batch   550  of  2,539.    Elapsed: 192.4789125919342.\n",
      "  Batch   600  of  2,539.    Elapsed: 210.0530731678009.\n",
      "  Batch   650  of  2,539.    Elapsed: 227.61681985855103.\n",
      "  Batch   700  of  2,539.    Elapsed: 245.3006091117859.\n",
      "  Batch   750  of  2,539.    Elapsed: 262.9939241409302.\n",
      "  Batch   800  of  2,539.    Elapsed: 280.5956521034241.\n",
      "  Batch   850  of  2,539.    Elapsed: 298.3100275993347.\n",
      "  Batch   900  of  2,539.    Elapsed: 315.9449214935303.\n",
      "  Batch   950  of  2,539.    Elapsed: 333.5671286582947.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 351.2442162036896.\n",
      "  Batch 1,050  of  2,539.    Elapsed: 368.9018487930298.\n",
      "  Batch 1,100  of  2,539.    Elapsed: 386.6157703399658.\n",
      "  Batch 1,150  of  2,539.    Elapsed: 404.10491251945496.\n",
      "  Batch 1,200  of  2,539.    Elapsed: 421.76475977897644.\n",
      "  Batch 1,250  of  2,539.    Elapsed: 439.2153775691986.\n",
      "  Batch 1,300  of  2,539.    Elapsed: 456.64925813674927.\n",
      "  Batch 1,350  of  2,539.    Elapsed: 474.2398784160614.\n",
      "  Batch 1,400  of  2,539.    Elapsed: 491.71039295196533.\n",
      "  Batch 1,450  of  2,539.    Elapsed: 508.95693373680115.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 526.1886053085327.\n",
      "  Batch 1,550  of  2,539.    Elapsed: 543.3917062282562.\n",
      "  Batch 1,600  of  2,539.    Elapsed: 560.726823091507.\n",
      "  Batch 1,650  of  2,539.    Elapsed: 577.99853515625.\n",
      "  Batch 1,700  of  2,539.    Elapsed: 595.217809677124.\n",
      "  Batch 1,750  of  2,539.    Elapsed: 612.5476055145264.\n",
      "  Batch 1,800  of  2,539.    Elapsed: 629.9458575248718.\n",
      "  Batch 1,850  of  2,539.    Elapsed: 647.2242305278778.\n",
      "  Batch 1,900  of  2,539.    Elapsed: 664.4961619377136.\n",
      "  Batch 1,950  of  2,539.    Elapsed: 681.8766474723816.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 699.0868577957153.\n",
      "  Batch 2,050  of  2,539.    Elapsed: 716.3479311466217.\n",
      "  Batch 2,100  of  2,539.    Elapsed: 733.7366223335266.\n",
      "  Batch 2,150  of  2,539.    Elapsed: 750.9316864013672.\n",
      "  Batch 2,200  of  2,539.    Elapsed: 768.2941408157349.\n",
      "  Batch 2,250  of  2,539.    Elapsed: 785.5913374423981.\n",
      "  Batch 2,300  of  2,539.    Elapsed: 802.8980543613434.\n",
      "  Batch 2,350  of  2,539.    Elapsed: 820.2370762825012.\n",
      "  Batch 2,400  of  2,539.    Elapsed: 837.4600918292999.\n",
      "  Batch 2,450  of  2,539.    Elapsed: 854.7447679042816.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 872.2267622947693.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epoc h took: 885.5417709350586\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.22992\n",
      "  F1: 0.22806\n",
      "  Validation Loss: 5.85078\n",
      "  Validation took: 93.35777854919434\n",
      "\n",
      "Training...\n",
      "----- Epoch 10 / 10 -----\n",
      "  Batch    50  of  2,539.    Elapsed: 17.2200870513916.\n",
      "  Batch   100  of  2,539.    Elapsed: 34.41997218132019.\n",
      "  Batch   150  of  2,539.    Elapsed: 51.68720459938049.\n",
      "  Batch   200  of  2,539.    Elapsed: 69.02781176567078.\n",
      "  Batch   250  of  2,539.    Elapsed: 86.35009717941284.\n",
      "  Batch   300  of  2,539.    Elapsed: 103.60851168632507.\n",
      "  Batch   350  of  2,539.    Elapsed: 120.89220261573792.\n",
      "  Batch   400  of  2,539.    Elapsed: 138.20343828201294.\n",
      "  Batch   450  of  2,539.    Elapsed: 155.5508532524109.\n",
      "  Batch   500  of  2,539.    Elapsed: 172.86725902557373.\n",
      "  Batch   550  of  2,539.    Elapsed: 190.1107954978943.\n",
      "  Batch   600  of  2,539.    Elapsed: 207.38344740867615.\n",
      "  Batch   650  of  2,539.    Elapsed: 224.58001041412354.\n",
      "  Batch   700  of  2,539.    Elapsed: 241.83830213546753.\n",
      "  Batch   750  of  2,539.    Elapsed: 259.09305810928345.\n",
      "  Batch   800  of  2,539.    Elapsed: 276.33327054977417.\n",
      "  Batch   850  of  2,539.    Elapsed: 293.68183517456055.\n",
      "  Batch   900  of  2,539.    Elapsed: 310.91803312301636.\n",
      "  Batch   950  of  2,539.    Elapsed: 328.2790253162384.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.7809314727783.\n",
      "  Batch 1,050  of  2,539.    Elapsed: 363.0818133354187.\n",
      "  Batch 1,100  of  2,539.    Elapsed: 380.39777994155884.\n",
      "  Batch 1,150  of  2,539.    Elapsed: 397.8519310951233.\n",
      "  Batch 1,200  of  2,539.    Elapsed: 415.0703778266907.\n",
      "  Batch 1,250  of  2,539.    Elapsed: 432.3473479747772.\n",
      "  Batch 1,300  of  2,539.    Elapsed: 449.7975609302521.\n",
      "  Batch 1,350  of  2,539.    Elapsed: 467.38690662384033.\n",
      "  Batch 1,400  of  2,539.    Elapsed: 485.21293473243713.\n",
      "  Batch 1,450  of  2,539.    Elapsed: 502.8841116428375.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 520.3284451961517.\n",
      "  Batch 1,550  of  2,539.    Elapsed: 538.1286935806274.\n",
      "  Batch 1,600  of  2,539.    Elapsed: 555.7786231040955.\n",
      "  Batch 1,650  of  2,539.    Elapsed: 573.7031061649323.\n",
      "  Batch 1,700  of  2,539.    Elapsed: 591.6274886131287.\n",
      "  Batch 1,750  of  2,539.    Elapsed: 609.4754981994629.\n",
      "  Batch 1,800  of  2,539.    Elapsed: 627.6675152778625.\n",
      "  Batch 1,850  of  2,539.    Elapsed: 645.5518133640289.\n",
      "  Batch 1,900  of  2,539.    Elapsed: 663.1462850570679.\n",
      "  Batch 1,950  of  2,539.    Elapsed: 681.0159120559692.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 698.3970992565155.\n",
      "  Batch 2,050  of  2,539.    Elapsed: 715.8630974292755.\n",
      "  Batch 2,100  of  2,539.    Elapsed: 733.457291841507.\n",
      "  Batch 2,150  of  2,539.    Elapsed: 751.5765597820282.\n",
      "  Batch 2,200  of  2,539.    Elapsed: 769.3060035705566.\n",
      "  Batch 2,250  of  2,539.    Elapsed: 786.7609078884125.\n",
      "  Batch 2,300  of  2,539.    Elapsed: 804.5571761131287.\n",
      "  Batch 2,350  of  2,539.    Elapsed: 822.3986332416534.\n",
      "  Batch 2,400  of  2,539.    Elapsed: 839.9566605091095.\n",
      "  Batch 2,450  of  2,539.    Elapsed: 857.5182898044586.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 875.0469605922699.\n",
      "\n",
      "  Average training loss: 0.33\n",
      "  Training epoc h took: 888.5047264099121\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.23465\n",
      "  F1: 0.22924\n",
      "  Validation Loss: 6.75319\n",
      "  Validation took: 94.60319399833679\n"
     ]
    }
   ],
   "source": [
    "training_stats = []\n",
    "validations_labels_ep = []\n",
    "actual_labels_ep = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "for i in range(0, num_epochs):\n",
    "    print('')\n",
    "    print('Training...')\n",
    "    print('----- Epoch {:} / {:} -----'.format(i + 1, num_epochs))\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device).to(torch.int64)\n",
    "        b_input_mask = batch[1].to(device).to(torch.int64)\n",
    "        b_labels = batch[2].to(device).to(torch.int64)\n",
    "        \n",
    "        model.zero_grad()        \n",
    "\n",
    "        loss = model(b_input_ids, \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=b_input_mask,\n",
    "                     labels=b_labels)[0]\n",
    "        logits = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask,\n",
    "                       labels=b_labels)[1]\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    training_time = time.time() - t0\n",
    "\n",
    "    print('')\n",
    "    print('  Average training loss: {0:.2f}'.format(avg_train_loss))\n",
    "    print('  Training epoc h took: {:}'.format(training_time))\n",
    "    \n",
    "    print('')\n",
    "    print('Running Validation...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    total_eval_f1 = 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            loss = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)[0]\n",
    "\n",
    "            logits = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)[1]\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        \n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU:\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches:\n",
    "        \n",
    "        total_eval_accuracy += accuracy(logits, label_ids)\n",
    "        total_eval_f1 += flat_f1_score(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print('  Accuracy: {0:.5f}'.format(avg_val_accuracy))\n",
    "    \n",
    "    # Report the final f1 score for this validation run.\n",
    "    \n",
    "    avg_val_f1 = total_eval_f1 / len(validation_dataloader)\n",
    "    print('  F1: {0:.5f}'.format(avg_val_f1))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    \n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Measure how long the validation run took:\n",
    "    \n",
    "    validation_time = time.time() - t0\n",
    "    \n",
    "    print('  Validation Loss: {0:.5f}'.format(avg_val_loss))\n",
    "    print('  Validation took: {:}'.format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    \n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Val_F1' : avg_val_f1,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b180fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"./models/bert_{CUR_DATASET}_regexp_stopwords_{if_stopwords}_lemmatization_{if_lemmatize}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp-transformers] *",
   "language": "python",
   "name": "conda-env-nlp-transformers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
